{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b87b60-fe40-411d-86ac-ba09eb35cae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100-SXM4-40GB\n",
      "allocated: 0.0 MB\n",
      "reserved: 0.0 MB\n",
      "allocated: 0.0 MB\n",
      "reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1 — ENV (MUST be first, before torch/transformers)\n",
    "# =========================\n",
    "from ft_pipeline.env import apply_env\n",
    "apply_env()\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n",
    "\n",
    "\n",
    "\n",
    "import gc, torch\n",
    "print(\"allocated:\", torch.cuda.memory_allocated()/1024**2, \"MB\")\n",
    "print(\"reserved:\",  torch.cuda.memory_reserved()/1024**2, \"MB\")\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"allocated:\", torch.cuda.memory_allocated()/1024**2, \"MB\")\n",
    "print(\"reserved:\",  torch.cuda.memory_reserved()/1024**2, \"MB\")\n",
    "\n",
    "import logging\n",
    "from ft_pipeline.logger import setup_logger\n",
    "from ft_pipeline.config import FTConfig\n",
    "from ft_pipeline.run_sft import run_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "219c865a-2fc7-4ec2-b2a7-b833f240c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTConfig(model_id='/home/jovyan/ai-models/MamayLM-Gemma-3-12B', train_jsonl='ft_datasets/sft_train.jsonl', val_jsonl='ft_datasets/sft_val.jsonl', out_dir='MamayLM-Gemma-3-12b_QLoRA_SFT', max_seq_len=5000, per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=8, learning_rate=8e-07, weight_decay=0.01, num_train_epochs=2, max_steps=None, warmup_ratio=0.05, lr_scheduler_type='cosine', logging_steps=5, eval_steps=50, save_steps=200, save_total_limit=2, use_bf16=True, use_fp16=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, attn_implementation='sdpa', lora_r=16, lora_alpha=32, lora_dropout=0.05, target_modules=None, packing=False, optim='paged_adamw_8bit', report_to='none', max_new_tokens_eval=512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Logger ft_pipeline (INFO)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2 — FTConfig \n",
    "# =========================\n",
    "cfg = FTConfig(\n",
    "    # -----------------------\n",
    "    # PATHS / INPUT-OUTPUT\n",
    "    # -----------------------\n",
    "    model_id=\"/home/jovyan/ai-models/MamayLM-Gemma-3-12B\",  # path or HF repo id base model\n",
    "    train_jsonl=\"ft_datasets/sft_train.jsonl\",             # train dataset in JSONL\n",
    "    val_jsonl=\"ft_datasets/sft_val.jsonl\",                 # validation dataset in JSONL\n",
    "    out_dir=\"MamayLM-Gemma-3-12b_QLoRA_SFT\",         \n",
    "\n",
    "    # -----------------------\n",
    "    # SEQUENCE / BATCHING\n",
    "    # -----------------------\n",
    "    max_seq_len=5000,                 # max context (promt+complections)\n",
    "    per_device_train_batch_size=1,    # batch size на GPU \n",
    "    per_device_eval_batch_size=1,     # batch size на eval \n",
    "    gradient_accumulation_steps=8,    # (effective batch = batch_size * grad_accum)\n",
    "\n",
    "    # -----------------------\n",
    "    # TRAINING SCHEDULE / OPTIM\n",
    "    # -----------------------\n",
    "    learning_rate=0.0000008,          \n",
    "    num_train_epochs=2,               # (if  max_steps are provided - will ignored)\n",
    "    max_steps=None, #400 None         # use instead - num_train_epochs\n",
    "    warmup_ratio=0.05,              \n",
    "    lr_scheduler_type=\"cosine\",       # scheduler: \"cosine\", \"linear\", ...\n",
    "    logging_steps=5,                  # how often to log (steps)\n",
    "    eval_steps=50,                    # how often to eval (steps)\n",
    "    save_steps=200,                   # how often to save checkpoint (steps)\n",
    "    save_total_limit=2,               # how many checkpoint to save\n",
    "    weight_decay=0.01,                # L2 regularization \n",
    "\n",
    "    use_bf16=True,                    # BF16 (A100 — best)\n",
    "    use_fp16=False,                   # fallback FP16 (if bf16 unavailable)\n",
    "\n",
    "    # -----------------------\n",
    "    # QLORA / BNB (4-bit quant)\n",
    "    # -----------------------\n",
    "    load_in_4bit=True,                # QLoRA 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",        # quantization (nf4 — standart)\n",
    "    bnb_4bit_use_double_quant=True,   # double-quant (often is good for quolity)\n",
    "    attn_implementation=\"sdpa\",       # \"sdpa\" — stable; \n",
    "\n",
    "    # -----------------------\n",
    "    # LORA (adapter)\n",
    "    # -----------------------\n",
    "    lora_r=16,                        # rank\n",
    "    lora_alpha=32,                    # scaling (often 2*r or 4*r for SFT)\n",
    "    lora_dropout=0.05,                # dropout in LoRA\n",
    "    target_modules=None,              # None → default in resolved_target_modules()\n",
    "\n",
    "    # -----------------------\n",
    "    # TRAINER BEHAVIOR\n",
    "    # -----------------------\n",
    "    packing=False,                    # packing a few samples in one seq \n",
    "    optim=\"paged_adamw_8bit\",         # optimizator (8bit AdamW з bitsandbytes)\n",
    "    report_to=\"none\",                 # \"none\", \"wandb\", ...\n",
    "\n",
    "    # -----------------------\n",
    "    # INFERENCE SANITY CHECKS\n",
    "    # -----------------------\n",
    "    max_new_tokens_eval=512,          #  in A/B sanity (before/after)\n",
    ")\n",
    "\n",
    "print(cfg)\n",
    "setup_logger(level=logging.INFO, log_file=f\"{cfg.out_dir}/ft_run_sft.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789a52a-609f-4a82-a35e-bb12f4c578b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:58:30 | INFO    | === FT RUN START ===\n",
      "12:58:30 | INFO    | CUDA available=True\n",
      "12:58:30 | INFO    | CUDA device=NVIDIA A100-SXM4-40GB\n",
      "12:58:30 | INFO    | Loading datasets\n",
      "12:58:30 | INFO    |   train: ft_datasets/sft_train.jsonl\n",
      "12:58:30 | INFO    |   val:   ft_datasets/sft_val.jsonl\n",
      "12:58:55 | INFO    | Converting to prompt/completion format\n",
      "12:58:55 | INFO    | Dataset ready | train=3059 | val=340\n",
      "12:58:55 | INFO    | Loading tokenizer: /home/jovyan/ai-models/MamayLM-Gemma-3-12B\n",
      "12:58:56 | INFO    | Tokenizer loaded\n",
      "12:58:56 | INFO    | Loading base model (QLoRA)\n",
      "12:58:56 | INFO    |   model_id: /home/jovyan/ai-models/MamayLM-Gemma-3-12B\n",
      "12:58:56 | INFO    |   dtype: torch.bfloat16\n",
      "12:58:56 | INFO    |   4bit: True\n",
      "12:58:56 | INFO    |   attn_implementation: sdpa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bdd861791345fa8568642b4c9d1270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:00:05 | INFO    | Base model loaded\n",
      "13:00:05 | INFO    | Enabling gradient checkpointing\n",
      "13:00:05 | INFO    | Applying LoRA\n",
      "13:00:05 | INFO    |   r=16, alpha=32, dropout=0.05\n",
      "13:00:05 | INFO    |   target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj', 'gate_proj']\n",
      "13:00:06 | INFO    | LoRA applied successfully\n",
      "13:00:06 | INFO    | Trainable parameters:\n",
      "trainable params: 68,456,448 || all params: 12,255,781,488 || trainable%: 0.5586\n",
      "13:00:06 | INFO    | Building SFTConfig\n",
      "13:00:06 | INFO    |   max_seq_len=5000\n",
      "13:00:06 | INFO    |   batch_size=1\n",
      "13:00:06 | INFO    |   grad_accum=8\n",
      "13:00:06 | INFO    |   lr=8e-07\n",
      "13:00:06 | INFO    | Building SFTTrainer\n",
      "13:00:06 | INFO    |   train_samples=3059\n",
      "13:00:06 | INFO    |   val_samples=340\n",
      "13:00:06 | INFO    |   dataset_mode=prompt_completion\n",
      "13:00:06 | INFO    |   max_seq_length=5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:00:07 | INFO    | labels shape: (1, 3136)\n",
      "13:00:07 | INFO    | non -100 labels: 261\n",
      "13:00:07 | INFO    | Starting training…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipped Embedding(4096, 1152): 4.5M params\n",
      "skipped Gemma3TextScaledWordEmbedding(262208, 3840, padding_idx=0): 964.734375M params\n",
      "skipped: 964.734375M params\n",
      "***** Running training *****\n",
      "  Num examples = 3,059\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 766\n",
      "  Number of trainable parameters = 68,456,448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:00:36 | INFO    | GPUMetricsCallback enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='551' max='766' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [551/766 8:23:35 < 3:17:13, 0.02 it/s, Epoch 1.44/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.314500</td>\n",
       "      <td>0.536637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.513900</td>\n",
       "      <td>0.440426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.735500</td>\n",
       "      <td>0.339168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.008800</td>\n",
       "      <td>0.251918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.493000</td>\n",
       "      <td>0.183378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.204500</td>\n",
       "      <td>0.132083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.852400</td>\n",
       "      <td>0.107348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.646600</td>\n",
       "      <td>0.093778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.802100</td>\n",
       "      <td>0.085384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.079573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/340 02:23 < 05:07, 0.75 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:01:22 | INFO    | [step 1] train_loss=4.9221 | lr=0 | grad_norm=8.3960 | train_tok=seq_mean:3075 prompt_mean:2831 loss_mean:249 loss_max:249 | gpu_mem(GB)=alloc:9.49 res:23.73 max_alloc:23.21 max_res:38.98 | elapsed=0.8m\n",
      "13:04:23 | INFO    | [step 5] train_loss=4.6771 | lr=8.20513e-08 | grad_norm=10.5648 | train_tok=seq_mean:3137 prompt_mean:2850 loss_mean:294 loss_max:294 | gpu_mem(GB)=alloc:9.49 res:19.05 max_alloc:23.38 max_res:38.98 | elapsed=3.8m\n",
      "13:08:18 | INFO    | [step 10] train_loss=4.5414 | lr=1.84615e-07 | grad_norm=15.6103 | train_tok=seq_mean:2795 prompt_mean:2485 loss_mean:315 loss_max:315 | gpu_mem(GB)=alloc:9.49 res:36.32 max_alloc:23.38 max_res:38.98 | elapsed=7.7m\n",
      "13:12:08 | INFO    | [step 15] train_loss=4.5811 | lr=2.87179e-07 | grad_norm=8.5295 | train_tok=seq_mean:3113 prompt_mean:2836 loss_mean:284 loss_max:284 | gpu_mem(GB)=alloc:9.49 res:32.06 max_alloc:23.38 max_res:38.98 | elapsed=11.5m\n",
      "13:16:02 | INFO    | [step 20] train_loss=4.7831 | lr=3.89744e-07 | grad_norm=13.1003 | train_tok=seq_mean:3093 prompt_mean:2843 loss_mean:253 loss_max:253 | gpu_mem(GB)=alloc:9.49 res:22.45 max_alloc:23.45 max_res:38.98 | elapsed=15.4m\n",
      "13:19:45 | INFO    | [step 25] train_loss=4.5828 | lr=4.92308e-07 | grad_norm=8.3635 | train_tok=seq_mean:3096 prompt_mean:2845 loss_mean:251 loss_max:251 | gpu_mem(GB)=alloc:9.49 res:26.12 max_alloc:23.45 max_res:38.98 | elapsed=19.2m\n",
      "13:23:37 | INFO    | [step 30] train_loss=4.4396 | lr=5.94872e-07 | grad_norm=16.6530 | train_tok=seq_mean:2953 prompt_mean:2702 loss_mean:258 loss_max:258 | gpu_mem(GB)=alloc:9.49 res:32.78 max_alloc:23.45 max_res:38.98 | elapsed=23.0m\n",
      "13:27:19 | INFO    | [step 35] train_loss=4.4333 | lr=6.97436e-07 | grad_norm=8.3159 | train_tok=seq_mean:2974 prompt_mean:2718 loss_mean:258 loss_max:258 | gpu_mem(GB)=alloc:9.49 res:15.77 max_alloc:23.45 max_res:38.98 | elapsed=26.7m\n",
      "13:31:09 | INFO    | [step 40] train_loss=4.3281 | lr=8e-07 | grad_norm=9.2811 | train_tok=seq_mean:3116 prompt_mean:2874 loss_mean:246 loss_max:246 | gpu_mem(GB)=alloc:9.49 res:22.47 max_alloc:23.45 max_res:38.98 | elapsed=30.5m\n",
      "13:34:56 | INFO    | [step 45] train_loss=4.5367 | lr=7.99907e-07 | grad_norm=9.9958 | train_tok=seq_mean:3168 prompt_mean:2877 loss_mean:291 loss_max:291 | gpu_mem(GB)=alloc:9.49 res:24.87 max_alloc:23.45 max_res:38.98 | elapsed=34.3m\n",
      "13:38:55 | INFO    | [step 50] train_loss=4.3145 | lr=7.99627e-07 | grad_norm=9.7690 | train_tok=seq_mean:2542 prompt_mean:2263 loss_mean:281 loss_max:281 | gpu_mem(GB)=alloc:9.49 res:38.04 max_alloc:23.45 max_res:38.98 | elapsed=38.3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 340\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:46:44 | INFO    | [step 50] eval_loss=0.5366 | train_tok=seq_mean:2576 prompt_mean:2284 loss_mean:292 loss_max:292 | gpu_mem(GB)=alloc:9.49 res:37.84 max_alloc:23.45 max_res:38.98 | elapsed=46.1m\n",
      "13:46:44 | INFO    | EarlyStop(metric=eval_loss): improved from None to 0.536637\n",
      "13:50:29 | INFO    | [step 55] train_loss=4.1769 | lr=7.9916e-07 | grad_norm=8.9928 | train_tok=seq_mean:2798 prompt_mean:2475 loss_mean:325 loss_max:325 | gpu_mem(GB)=alloc:9.49 res:27.12 max_alloc:23.45 max_res:38.98 | elapsed=49.9m\n",
      "13:54:28 | INFO    | [step 60] train_loss=4.1384 | lr=7.98507e-07 | grad_norm=10.1909 | train_tok=seq_mean:2998 prompt_mean:2710 loss_mean:290 loss_max:290 | gpu_mem(GB)=alloc:9.49 res:21.26 max_alloc:23.45 max_res:38.98 | elapsed=53.9m\n",
      "13:58:30 | INFO    | [step 65] train_loss=4.0595 | lr=7.97668e-07 | grad_norm=6.9578 | train_tok=seq_mean:3118 prompt_mean:2875 loss_mean:245 loss_max:245 | gpu_mem(GB)=alloc:9.49 res:11.76 max_alloc:23.45 max_res:38.98 | elapsed=57.9m\n",
      "14:02:21 | INFO    | [step 70] train_loss=4.0432 | lr=7.96643e-07 | grad_norm=8.0660 | train_tok=seq_mean:3119 prompt_mean:2870 loss_mean:250 loss_max:250 | gpu_mem(GB)=alloc:9.49 res:19.71 max_alloc:23.45 max_res:38.98 | elapsed=61.7m\n",
      "14:06:14 | INFO    | [step 75] train_loss=3.9748 | lr=7.95434e-07 | grad_norm=11.8036 | train_tok=seq_mean:3092 prompt_mean:2841 loss_mean:255 loss_max:255 | gpu_mem(GB)=alloc:9.49 res:15.63 max_alloc:23.45 max_res:38.98 | elapsed=65.6m\n",
      "14:10:12 | INFO    | [step 80] train_loss=3.8957 | lr=7.94039e-07 | grad_norm=8.1503 | train_tok=seq_mean:2574 prompt_mean:2287 loss_mean:289 loss_max:289 | gpu_mem(GB)=alloc:9.49 res:26.41 max_alloc:23.45 max_res:38.98 | elapsed=69.6m\n",
      "14:13:54 | INFO    | [step 85] train_loss=3.8358 | lr=7.92461e-07 | grad_norm=9.7373 | train_tok=seq_mean:3108 prompt_mean:2852 loss_mean:260 loss_max:260 | gpu_mem(GB)=alloc:9.49 res:29.79 max_alloc:23.45 max_res:38.98 | elapsed=73.3m\n",
      "14:17:52 | INFO    | [step 90] train_loss=3.7322 | lr=7.90699e-07 | grad_norm=8.0309 | train_tok=seq_mean:3122 prompt_mean:2880 loss_mean:248 loss_max:248 | gpu_mem(GB)=alloc:9.49 res:37.14 max_alloc:23.45 max_res:38.98 | elapsed=77.3m\n",
      "14:21:53 | INFO    | [step 95] train_loss=3.6750 | lr=7.88755e-07 | grad_norm=7.8242 | train_tok=seq_mean:3118 prompt_mean:2869 loss_mean:251 loss_max:251 | gpu_mem(GB)=alloc:9.49 res:22.84 max_alloc:23.45 max_res:38.98 | elapsed=81.3m\n",
      "14:25:52 | INFO    | [step 100] train_loss=3.5139 | lr=7.8663e-07 | grad_norm=7.3008 | train_tok=seq_mean:3150 prompt_mean:2872 loss_mean:280 loss_max:280 | gpu_mem(GB)=alloc:9.49 res:15.89 max_alloc:23.45 max_res:38.98 | elapsed=85.3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 340\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:33:40 | INFO    | [step 100] eval_loss=0.4404 | train_tok=seq_mean:2576 prompt_mean:2284 loss_mean:292 loss_max:292 | gpu_mem(GB)=alloc:9.49 res:37.84 max_alloc:23.45 max_res:38.98 | elapsed=93.1m\n",
      "14:33:40 | INFO    | EarlyStop(metric=eval_loss): improved from 0.536637 to 0.440426\n",
      "14:37:41 | INFO    | [step 105] train_loss=3.3891 | lr=7.84324e-07 | grad_norm=7.9614 | train_tok=seq_mean:3124 prompt_mean:2875 loss_mean:253 loss_max:253 | gpu_mem(GB)=alloc:9.49 res:30.59 max_alloc:23.45 max_res:38.98 | elapsed=97.1m\n",
      "14:41:36 | INFO    | [step 110] train_loss=3.3781 | lr=7.81839e-07 | grad_norm=6.9335 | train_tok=seq_mean:3132 prompt_mean:2880 loss_mean:256 loss_max:256 | gpu_mem(GB)=alloc:9.49 res:30.01 max_alloc:23.45 max_res:38.98 | elapsed=101.0m\n",
      "14:45:27 | INFO    | [step 115] train_loss=3.2720 | lr=7.79175e-07 | grad_norm=7.4506 | train_tok=seq_mean:2913 prompt_mean:2606 loss_mean:314 loss_max:314 | gpu_mem(GB)=alloc:9.49 res:14.91 max_alloc:23.45 max_res:38.98 | elapsed=104.8m\n",
      "14:49:20 | INFO    | [step 120] train_loss=3.2759 | lr=7.76335e-07 | grad_norm=14.4803 | train_tok=seq_mean:3099 prompt_mean:2816 loss_mean:288 loss_max:288 | gpu_mem(GB)=alloc:9.49 res:23.02 max_alloc:23.45 max_res:38.98 | elapsed=108.7m\n",
      "14:53:17 | INFO    | [step 125] train_loss=3.2272 | lr=7.73318e-07 | grad_norm=7.5954 | train_tok=seq_mean:3092 prompt_mean:2846 loss_mean:250 loss_max:250 | gpu_mem(GB)=alloc:9.49 res:21.42 max_alloc:23.45 max_res:38.98 | elapsed=112.7m\n",
      "14:57:05 | INFO    | [step 130] train_loss=3.0474 | lr=7.70128e-07 | grad_norm=7.8087 | train_tok=seq_mean:3096 prompt_mean:2847 loss_mean:249 loss_max:249 | gpu_mem(GB)=alloc:9.49 res:18.68 max_alloc:23.45 max_res:38.98 | elapsed=116.5m\n",
      "15:00:53 | INFO    | [step 135] train_loss=2.9062 | lr=7.66765e-07 | grad_norm=6.7949 | train_tok=seq_mean:3117 prompt_mean:2875 loss_mean:245 loss_max:245 | gpu_mem(GB)=alloc:9.49 res:26.77 max_alloc:23.45 max_res:38.98 | elapsed=120.3m\n",
      "15:04:43 | INFO    | [step 140] train_loss=2.8148 | lr=7.6323e-07 | grad_norm=7.4875 | train_tok=seq_mean:2564 prompt_mean:2281 loss_mean:287 loss_max:287 | gpu_mem(GB)=alloc:9.49 res:32.68 max_alloc:23.45 max_res:38.98 | elapsed=124.1m\n",
      "15:08:32 | INFO    | [step 145] train_loss=2.8290 | lr=7.59526e-07 | grad_norm=7.1347 | train_tok=seq_mean:3100 prompt_mean:2848 loss_mean:256 loss_max:256 | gpu_mem(GB)=alloc:9.49 res:19.79 max_alloc:23.45 max_res:38.98 | elapsed=127.9m\n",
      "15:12:30 | INFO    | [step 150] train_loss=2.7355 | lr=7.55654e-07 | grad_norm=7.8320 | train_tok=seq_mean:2716 prompt_mean:2428 loss_mean:292 loss_max:292 | gpu_mem(GB)=alloc:9.49 res:28.64 max_alloc:23.45 max_res:38.98 | elapsed=131.9m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 340\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:18 | INFO    | [step 150] eval_loss=0.3392 | train_tok=seq_mean:2576 prompt_mean:2284 loss_mean:292 loss_max:292 | gpu_mem(GB)=alloc:9.49 res:37.84 max_alloc:23.45 max_res:38.98 | elapsed=139.7m\n",
      "15:20:18 | INFO    | EarlyStop(metric=eval_loss): improved from 0.440426 to 0.339168\n",
      "15:24:07 | INFO    | [step 155] train_loss=2.6827 | lr=7.51616e-07 | grad_norm=7.3670 | train_tok=seq_mean:3125 prompt_mean:2846 loss_mean:282 loss_max:282 | gpu_mem(GB)=alloc:9.49 res:19.89 max_alloc:23.45 max_res:38.98 | elapsed=143.5m\n",
      "15:27:54 | INFO    | [step 160] train_loss=2.6633 | lr=7.47414e-07 | grad_norm=7.2646 | train_tok=seq_mean:3160 prompt_mean:2853 loss_mean:307 loss_max:307 | gpu_mem(GB)=alloc:9.49 res:20.24 max_alloc:23.50 max_res:38.98 | elapsed=147.3m\n",
      "15:31:50 | INFO    | [step 165] train_loss=2.6261 | lr=7.4305e-07 | grad_norm=8.2649 | train_tok=seq_mean:2768 prompt_mean:2464 loss_mean:304 loss_max:304 | gpu_mem(GB)=alloc:9.49 res:19.09 max_alloc:23.50 max_res:38.98 | elapsed=151.2m\n",
      "15:35:44 | INFO    | [step 170] train_loss=2.3547 | lr=7.38525e-07 | grad_norm=7.5558 | train_tok=seq_mean:3088 prompt_mean:2833 loss_mean:255 loss_max:255 | gpu_mem(GB)=alloc:9.49 res:34.66 max_alloc:23.50 max_res:38.98 | elapsed=155.1m\n",
      "15:39:45 | INFO    | [step 175] train_loss=2.4462 | lr=7.33843e-07 | grad_norm=6.9276 | train_tok=seq_mean:3187 prompt_mean:2884 loss_mean:308 loss_max:308 | gpu_mem(GB)=alloc:9.49 res:37.47 max_alloc:23.50 max_res:38.98 | elapsed=159.1m\n",
      "15:43:41 | INFO    | [step 180] train_loss=2.3241 | lr=7.29005e-07 | grad_norm=6.7539 | train_tok=seq_mean:3123 prompt_mean:2871 loss_mean:257 loss_max:257 | gpu_mem(GB)=alloc:9.49 res:31.90 max_alloc:23.50 max_res:38.98 | elapsed=163.1m\n",
      "15:47:31 | INFO    | [step 185] train_loss=2.2624 | lr=7.24013e-07 | grad_norm=6.8454 | train_tok=seq_mean:3119 prompt_mean:2868 loss_mean:252 loss_max:252 | gpu_mem(GB)=alloc:9.49 res:21.79 max_alloc:23.50 max_res:38.98 | elapsed=166.9m\n",
      "15:51:29 | INFO    | [step 190] train_loss=2.3191 | lr=7.1887e-07 | grad_norm=10.3717 | train_tok=seq_mean:3107 prompt_mean:2845 loss_mean:267 loss_max:267 | gpu_mem(GB)=alloc:9.49 res:11.71 max_alloc:23.50 max_res:38.98 | elapsed=170.9m\n",
      "15:55:20 | INFO    | [step 195] train_loss=2.1272 | lr=7.13578e-07 | grad_norm=6.8281 | train_tok=seq_mean:3115 prompt_mean:2873 loss_mean:247 loss_max:247 | gpu_mem(GB)=alloc:9.49 res:27.51 max_alloc:23.50 max_res:38.98 | elapsed=174.7m\n",
      "15:59:14 | INFO    | [step 200] train_loss=2.0088 | lr=7.08139e-07 | grad_norm=6.9607 | train_tok=seq_mean:2717 prompt_mean:2430 loss_mean:290 loss_max:290 | gpu_mem(GB)=alloc:9.49 res:16.53 max_alloc:23.50 max_res:38.98 | elapsed=178.6m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 340\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:07:04 | INFO    | [step 200] eval_loss=0.2519 | train_tok=seq_mean:2576 prompt_mean:2284 loss_mean:292 loss_max:292 | gpu_mem(GB)=alloc:9.49 res:37.84 max_alloc:23.50 max_res:38.98 | elapsed=186.5m\n",
      "16:07:04 | INFO    | EarlyStop(metric=eval_loss): improved from 0.339168 to 0.251918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MamayLM-Gemma-3-12b_QLoRA_SFT/checkpoint-200\n",
      "chat template saved in MamayLM-Gemma-3-12b_QLoRA_SFT/checkpoint-200/chat_template.jinja\n",
      "tokenizer config file saved in MamayLM-Gemma-3-12b_QLoRA_SFT/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in MamayLM-Gemma-3-12b_QLoRA_SFT/checkpoint-200/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:10:59 | INFO    | [step 205] train_loss=1.9281 | lr=7.02557e-07 | grad_norm=7.7062 | train_tok=seq_mean:2786 prompt_mean:2478 loss_mean:314 loss_max:314 | gpu_mem(GB)=alloc:9.49 res:28.50 max_alloc:23.50 max_res:38.98 | elapsed=190.4m\n",
      "16:14:46 | INFO    | [step 210] train_loss=2.0150 | lr=6.96833e-07 | grad_norm=6.4973 | train_tok=seq_mean:3161 prompt_mean:2874 loss_mean:294 loss_max:294 | gpu_mem(GB)=alloc:9.49 res:21.75 max_alloc:23.50 max_res:38.98 | elapsed=194.2m\n",
      "16:18:36 | INFO    | [step 215] train_loss=1.8412 | lr=6.90971e-07 | grad_norm=9.2631 | train_tok=seq_mean:3087 prompt_mean:2846 loss_mean:242 loss_max:242 | gpu_mem(GB)=alloc:9.49 res:25.07 max_alloc:23.50 max_res:38.98 | elapsed=198.0m\n",
      "16:22:27 | INFO    | [step 220] train_loss=1.9752 | lr=6.84974e-07 | grad_norm=7.2543 | train_tok=seq_mean:3107 prompt_mean:2850 loss_mean:262 loss_max:262 | gpu_mem(GB)=alloc:9.49 res:29.52 max_alloc:23.50 max_res:38.98 | elapsed=201.8m\n",
      "16:26:20 | INFO    | [step 225] train_loss=1.7226 | lr=6.78843e-07 | grad_norm=6.7884 | train_tok=seq_mean:3123 prompt_mean:2846 loss_mean:282 loss_max:282 | gpu_mem(GB)=alloc:9.49 res:23.25 max_alloc:23.50 max_res:38.98 | elapsed=205.7m\n",
      "16:30:15 | INFO    | [step 230] train_loss=1.7403 | lr=6.72582e-07 | grad_norm=6.5788 | train_tok=seq_mean:2764 prompt_mean:2446 loss_mean:322 loss_max:322 | gpu_mem(GB)=alloc:9.49 res:30.52 max_alloc:23.50 max_res:38.98 | elapsed=209.6m\n",
      "16:34:08 | INFO    | [step 235] train_loss=1.5925 | lr=6.66193e-07 | grad_norm=7.2489 | train_tok=seq_mean:3090 prompt_mean:2840 loss_mean:256 loss_max:256 | gpu_mem(GB)=alloc:9.49 res:27.92 max_alloc:23.50 max_res:38.98 | elapsed=213.5m\n",
      "16:38:06 | INFO    | [step 240] train_loss=1.4445 | lr=6.59681e-07 | grad_norm=7.5976 | train_tok=seq_mean:2772 prompt_mean:2465 loss_mean:311 loss_max:311 | gpu_mem(GB)=alloc:9.49 res:30.01 max_alloc:23.50 max_res:38.98 | elapsed=217.5m\n",
      "16:42:01 | INFO    | [step 245] train_loss=1.5460 | lr=6.53047e-07 | grad_norm=8.9820 | train_tok=seq_mean:3137 prompt_mean:2863 loss_mean:281 loss_max:281 | gpu_mem(GB)=alloc:9.49 res:35.09 max_alloc:23.50 max_res:38.98 | elapsed=221.4m\n",
      "16:45:55 | INFO    | [step 250] train_loss=1.4930 | lr=6.46295e-07 | grad_norm=7.7755 | train_tok=seq_mean:2783 prompt_mean:2459 loss_mean:325 loss_max:325 | gpu_mem(GB)=alloc:9.49 res:33.33 max_alloc:23.50 max_res:38.98 | elapsed=225.3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 340\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:53:43 | INFO    | [step 250] eval_loss=0.1834 | train_tok=seq_mean:2576 prompt_mean:2284 loss_mean:292 loss_max:292 | gpu_mem(GB)=alloc:9.49 res:37.84 max_alloc:23.50 max_res:38.98 | elapsed=233.1m\n",
      "16:53:43 | INFO    | EarlyStop(metric=eval_loss): improved from 0.251918 to 0.183378\n",
      "16:57:41 | INFO    | [step 255] train_loss=1.4068 | lr=6.39428e-07 | grad_norm=7.3153 | train_tok=seq_mean:3089 prompt_mean:2843 loss_mean:253 loss_max:253 | gpu_mem(GB)=alloc:9.49 res:29.68 max_alloc:23.50 max_res:38.98 | elapsed=237.1m\n",
      "17:01:27 | INFO    | [step 260] train_loss=1.4764 | lr=6.32449e-07 | grad_norm=7.4088 | train_tok=seq_mean:3071 prompt_mean:2831 loss_mean:241 loss_max:241 | gpu_mem(GB)=alloc:9.49 res:23.64 max_alloc:23.50 max_res:38.98 | elapsed=240.8m\n",
      "17:05:25 | INFO    | [step 265] train_loss=1.3997 | lr=6.25362e-07 | grad_norm=7.2752 | train_tok=seq_mean:3120 prompt_mean:2871 loss_mean:249 loss_max:249 | gpu_mem(GB)=alloc:9.49 res:16.59 max_alloc:23.50 max_res:38.98 | elapsed=244.8m\n",
      "17:09:10 | INFO    | [step 270] train_loss=1.2859 | lr=6.1817e-07 | grad_norm=8.0907 | train_tok=seq_mean:2918 prompt_mean:2611 loss_mean:309 loss_max:309 | gpu_mem(GB)=alloc:9.49 res:29.46 max_alloc:23.50 max_res:38.98 | elapsed=248.6m\n",
      "17:12:47 | INFO    | [step 275] train_loss=1.1869 | lr=6.10875e-07 | grad_norm=7.6858 | train_tok=seq_mean:2912 prompt_mean:2607 loss_mean:305 loss_max:305 | gpu_mem(GB)=alloc:9.49 res:22.76 max_alloc:23.50 max_res:38.98 | elapsed=252.2m\n",
      "17:16:54 | INFO    | [step 280] train_loss=1.2230 | lr=6.03483e-07 | grad_norm=7.2918 | train_tok=seq_mean:2712 prompt_mean:2418 loss_mean:294 loss_max:294 | gpu_mem(GB)=alloc:9.49 res:22.12 max_alloc:23.50 max_res:38.98 | elapsed=256.3m\n",
      "17:20:56 | INFO    | [step 285] train_loss=1.2406 | lr=5.95995e-07 | grad_norm=24.8852 | train_tok=seq_mean:2966 prompt_mean:2709 loss_mean:259 loss_max:259 | gpu_mem(GB)=alloc:9.49 res:27.43 max_alloc:23.50 max_res:38.98 | elapsed=260.3m\n",
      "17:24:55 | INFO    | [step 290] train_loss=1.2482 | lr=5.88416e-07 | grad_norm=6.9250 | train_tok=seq_mean:3123 prompt_mean:2872 loss_mean:256 loss_max:256 | gpu_mem(GB)=alloc:9.49 res:19.66 max_alloc:23.51 max_res:38.98 | elapsed=264.3m\n",
      "17:28:50 | INFO    | [step 295] train_loss=0.9301 | lr=5.80749e-07 | grad_norm=7.7661 | train_tok=seq_mean:3097 prompt_mean:2845 loss_mean:259 loss_max:259 | gpu_mem(GB)=alloc:9.49 res:23.99 max_alloc:23.54 max_res:38.98 | elapsed=268.2m\n",
      "17:32:34 | INFO    | [step 300] train_loss=1.2045 | lr=5.72997e-07 | grad_norm=6.9339 | train_tok=seq_mean:2757 prompt_mean:2456 loss_mean:304 loss_max:304 | gpu_mem(GB)=alloc:9.49 res:24.50 max_alloc:23.54 max_res:38.98 | elapsed=272.0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 340\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:40:22 | INFO    | [step 300] eval_loss=0.1321 | train_tok=seq_mean:2576 prompt_mean:2284 loss_mean:292 loss_max:292 | gpu_mem(GB)=alloc:9.49 res:37.84 max_alloc:23.54 max_res:38.98 | elapsed=279.8m\n",
      "17:40:22 | INFO    | EarlyStop(metric=eval_loss): improved from 0.183378 to 0.132083\n",
      "17:44:13 | INFO    | [step 305] train_loss=1.0120 | lr=5.65165e-07 | grad_norm=13.3096 | train_tok=seq_mean:2969 prompt_mean:2696 loss_mean:280 loss_max:280 | gpu_mem(GB)=alloc:9.49 res:23.74 max_alloc:23.54 max_res:38.98 | elapsed=283.6m\n",
      "17:48:06 | INFO    | [step 310] train_loss=0.9280 | lr=5.57255e-07 | grad_norm=9.5696 | train_tok=seq_mean:2571 prompt_mean:2279 loss_mean:297 loss_max:297 | gpu_mem(GB)=alloc:9.49 res:20.07 max_alloc:23.60 max_res:38.98 | elapsed=287.5m\n",
      "17:52:01 | INFO    | [step 315] train_loss=0.8929 | lr=5.49273e-07 | grad_norm=5.2648 | train_tok=seq_mean:3102 prompt_mean:2859 loss_mean:245 loss_max:245 | gpu_mem(GB)=alloc:9.49 res:36.16 max_alloc:23.60 max_res:38.98 | elapsed=291.4m\n",
      "17:55:51 | INFO    | [step 320] train_loss=0.8617 | lr=5.4122e-07 | grad_norm=5.7518 | train_tok=seq_mean:2887 prompt_mean:2582 loss_mean:306 loss_max:306 | gpu_mem(GB)=alloc:9.49 res:37.62 max_alloc:23.60 max_res:38.98 | elapsed=295.2m\n",
      "17:59:33 | INFO    | [step 325] train_loss=1.0475 | lr=5.33102e-07 | grad_norm=6.6714 | train_tok=seq_mean:2555 prompt_mean:2266 loss_mean:294 loss_max:294 | gpu_mem(GB)=alloc:9.49 res:30.83 max_alloc:23.60 max_res:38.98 | elapsed=299.0m\n",
      "18:03:29 | INFO    | [step 330] train_loss=0.9181 | lr=5.24921e-07 | grad_norm=6.0542 | train_tok=seq_mean:2958 prompt_mean:2705 loss_mean:255 loss_max:255 | gpu_mem(GB)=alloc:9.49 res:29.30 max_alloc:23.60 max_res:38.98 | elapsed=302.9m\n",
      "18:07:18 | INFO    | [step 335] train_loss=0.8756 | lr=5.16682e-07 | grad_norm=16.7355 | train_tok=seq_mean:2904 prompt_mean:2609 loss_mean:295 loss_max:295 | gpu_mem(GB)=alloc:9.49 res:36.69 max_alloc:23.60 max_res:38.98 | elapsed=306.7m\n",
      "18:11:04 | INFO    | [step 340] train_loss=0.8628 | lr=5.08389e-07 | grad_norm=7.2535 | train_tok=seq_mean:3083 prompt_mean:2827 loss_mean:261 loss_max:261 | gpu_mem(GB)=alloc:9.49 res:26.43 max_alloc:23.60 max_res:38.98 | elapsed=310.5m\n",
      "18:14:51 | INFO    | [step 345] train_loss=0.9556 | lr=5.00045e-07 | grad_norm=11.3513 | train_tok=seq_mean:3117 prompt_mean:2869 loss_mean:251 loss_max:251 | gpu_mem(GB)=alloc:9.49 res:11.63 max_alloc:23.60 max_res:38.98 | elapsed=314.2m\n",
      "18:18:45 | INFO    | [step 350] train_loss=0.8524 | lr=4.91654e-07 | grad_norm=10.2328 | train_tok=seq_mean:2964 prompt_mean:2712 loss_mean:256 loss_max:256 | gpu_mem(GB)=alloc:9.49 res:24.42 max_alloc:23.60 max_res:38.98 | elapsed=318.1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 340\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:26:33 | INFO    | [step 350] eval_loss=0.1073 | train_tok=seq_mean:2576 prompt_mean:2284 loss_mean:292 loss_max:292 | gpu_mem(GB)=alloc:9.49 res:37.84 max_alloc:23.60 max_res:38.98 | elapsed=325.9m\n",
      "18:26:33 | INFO    | EarlyStop(metric=eval_loss): improved from 0.132083 to 0.107348\n",
      "18:30:24 | INFO    | [step 355] train_loss=0.8125 | lr=4.83221e-07 | grad_norm=8.8585 | train_tok=seq_mean:3097 prompt_mean:2848 loss_mean:256 loss_max:256 | gpu_mem(GB)=alloc:9.49 res:18.27 max_alloc:23.60 max_res:38.98 | elapsed=329.8m\n",
      "18:34:17 | INFO    | [step 360] train_loss=0.8278 | lr=4.74749e-07 | grad_norm=7.9376 | train_tok=seq_mean:3124 prompt_mean:2873 loss_mean:255 loss_max:255 | gpu_mem(GB)=alloc:9.49 res:23.31 max_alloc:23.60 max_res:38.98 | elapsed=333.7m\n",
      "18:38:11 | INFO    | [step 365] train_loss=0.8669 | lr=4.66242e-07 | grad_norm=5.9211 | train_tok=seq_mean:3192 prompt_mean:2880 loss_mean:312 loss_max:312 | gpu_mem(GB)=alloc:9.49 res:19.95 max_alloc:23.60 max_res:38.98 | elapsed=337.6m\n",
      "18:42:07 | INFO    | [step 370] train_loss=0.9265 | lr=4.57704e-07 | grad_norm=8.0418 | train_tok=seq_mean:3123 prompt_mean:2873 loss_mean:255 loss_max:255 | gpu_mem(GB)=alloc:9.49 res:23.45 max_alloc:23.60 max_res:38.98 | elapsed=341.5m\n",
      "18:45:57 | INFO    | [step 375] train_loss=0.6869 | lr=4.49138e-07 | grad_norm=5.5039 | train_tok=seq_mean:3162 prompt_mean:2873 loss_mean:295 loss_max:295 | gpu_mem(GB)=alloc:9.49 res:16.94 max_alloc:23.60 max_res:38.98 | elapsed=345.3m\n",
      "18:49:51 | INFO    | [step 380] train_loss=0.7761 | lr=4.40551e-07 | grad_norm=8.4017 | train_tok=seq_mean:3100 prompt_mean:2839 loss_mean:265 loss_max:265 | gpu_mem(GB)=alloc:9.49 res:23.93 max_alloc:23.60 max_res:38.98 | elapsed=349.2m\n",
      "18:53:03 | INFO    | [step 385] train_loss=0.7137 | lr=4.31944e-07 | grad_norm=26.0911 | train_tok=seq_mean:3120 prompt_mean:2869 loss_mean:251 loss_max:251 | gpu_mem(GB)=alloc:9.49 res:29.89 max_alloc:23.60 max_res:38.98 | elapsed=352.4m\n",
      "18:57:01 | INFO    | [step 390] train_loss=0.6324 | lr=4.23322e-07 | grad_norm=6.5604 | train_tok=seq_mean:2983 prompt_mean:2690 loss_mean:294 loss_max:294 | gpu_mem(GB)=alloc:9.49 res:33.88 max_alloc:23.60 max_res:38.98 | elapsed=356.4m\n",
      "19:00:50 | INFO    | [step 395] train_loss=1.0033 | lr=4.14689e-07 | grad_norm=5.1420 | train_tok=seq_mean:3096 prompt_mean:2845 loss_mean:251 loss_max:251 | gpu_mem(GB)=alloc:9.49 res:26.86 max_alloc:23.60 max_res:38.98 | elapsed=360.2m\n",
      "19:04:40 | INFO    | [step 400] train_loss=0.6466 | lr=4.0605e-07 | grad_norm=5.6581 | train_tok=seq_mean:2592 prompt_mean:2297 loss_mean:295 loss_max:295 | gpu_mem(GB)=alloc:9.49 res:22.59 max_alloc:23.60 max_res:38.98 | elapsed=364.1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 340\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:12:30 | INFO    | [step 400] eval_loss=0.0938 | train_tok=seq_mean:2576 prompt_mean:2284 loss_mean:292 loss_max:292 | gpu_mem(GB)=alloc:9.49 res:37.84 max_alloc:23.60 max_res:38.98 | elapsed=371.9m\n",
      "19:12:30 | INFO    | EarlyStop(metric=eval_loss): improved from 0.107348 to 0.093778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MamayLM-Gemma-3-12b_QLoRA_SFT/checkpoint-400\n",
      "chat template saved in MamayLM-Gemma-3-12b_QLoRA_SFT/checkpoint-400/chat_template.jinja\n",
      "tokenizer config file saved in MamayLM-Gemma-3-12b_QLoRA_SFT/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in MamayLM-Gemma-3-12b_QLoRA_SFT/checkpoint-400/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:16:42 | INFO    | [step 405] train_loss=0.7380 | lr=3.97407e-07 | grad_norm=4.5856 | train_tok=seq_mean:3087 prompt_mean:2840 loss_mean:248 loss_max:248 | gpu_mem(GB)=alloc:9.49 res:18.13 max_alloc:23.60 max_res:38.98 | elapsed=376.1m\n",
      "19:20:41 | INFO    | [step 410] train_loss=0.6181 | lr=3.88766e-07 | grad_norm=6.8264 | train_tok=seq_mean:3123 prompt_mean:2845 loss_mean:283 loss_max:283 | gpu_mem(GB)=alloc:9.49 res:21.88 max_alloc:23.60 max_res:38.98 | elapsed=380.1m\n",
      "19:24:35 | INFO    | [step 415] train_loss=0.7249 | lr=3.8013e-07 | grad_norm=5.8120 | train_tok=seq_mean:2689 prompt_mean:2451 loss_mean:245 loss_max:245 | gpu_mem(GB)=alloc:9.49 res:24.46 max_alloc:23.60 max_res:38.98 | elapsed=384.0m\n",
      "19:28:22 | INFO    | [step 420] train_loss=0.6141 | lr=3.71504e-07 | grad_norm=8.7959 | train_tok=seq_mean:3165 prompt_mean:2874 loss_mean:294 loss_max:294 | gpu_mem(GB)=alloc:9.49 res:38.07 max_alloc:23.60 max_res:38.98 | elapsed=387.8m\n",
      "19:32:19 | INFO    | [step 425] train_loss=0.6183 | lr=3.6289e-07 | grad_norm=8.4715 | train_tok=seq_mean:2936 prompt_mean:2684 loss_mean:252 loss_max:252 | gpu_mem(GB)=alloc:9.49 res:15.83 max_alloc:23.60 max_res:38.98 | elapsed=391.7m\n",
      "19:36:19 | INFO    | [step 430] train_loss=0.6892 | lr=3.54294e-07 | grad_norm=4.9810 | train_tok=seq_mean:2789 prompt_mean:2480 loss_mean:312 loss_max:312 | gpu_mem(GB)=alloc:9.49 res:23.89 max_alloc:23.60 max_res:38.98 | elapsed=395.7m\n",
      "19:40:06 | INFO    | [step 435] train_loss=0.6061 | lr=3.45719e-07 | grad_norm=4.5117 | train_tok=seq_mean:3129 prompt_mean:2876 loss_mean:260 loss_max:260 | gpu_mem(GB)=alloc:9.49 res:29.93 max_alloc:23.60 max_res:38.98 | elapsed=399.5m\n",
      "19:43:55 | INFO    | [step 440] train_loss=0.6119 | lr=3.3717e-07 | grad_norm=6.8867 | train_tok=seq_mean:3093 prompt_mean:2838 loss_mean:258 loss_max:258 | gpu_mem(GB)=alloc:9.49 res:14.21 max_alloc:23.60 max_res:38.98 | elapsed=403.3m\n",
      "19:47:51 | INFO    | [step 445] train_loss=0.7376 | lr=3.2865e-07 | grad_norm=8.8061 | train_tok=seq_mean:2693 prompt_mean:2455 loss_mean:241 loss_max:241 | gpu_mem(GB)=alloc:9.49 res:19.97 max_alloc:23.60 max_res:38.98 | elapsed=407.2m\n",
      "19:51:51 | INFO    | [step 450] train_loss=0.8021 | lr=3.20163e-07 | grad_norm=5.2815 | train_tok=seq_mean:3126 prompt_mean:2876 loss_mean:252 loss_max:252 | gpu_mem(GB)=alloc:9.49 res:28.33 max_alloc:23.60 max_res:38.98 | elapsed=411.2m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 340\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:59:40 | INFO    | [step 450] eval_loss=0.0854 | train_tok=seq_mean:2576 prompt_mean:2284 loss_mean:292 loss_max:292 | gpu_mem(GB)=alloc:9.49 res:37.84 max_alloc:23.60 max_res:38.98 | elapsed=419.1m\n",
      "19:59:40 | INFO    | EarlyStop(metric=eval_loss): improved from 0.093778 to 0.085384\n",
      "20:03:34 | INFO    | [step 455] train_loss=0.6146 | lr=3.11714e-07 | grad_norm=5.7288 | train_tok=seq_mean:2914 prompt_mean:2610 loss_mean:310 loss_max:310 | gpu_mem(GB)=alloc:9.49 res:25.59 max_alloc:23.60 max_res:38.98 | elapsed=423.0m\n",
      "20:07:27 | INFO    | [step 460] train_loss=0.8082 | lr=3.03306e-07 | grad_norm=6.7957 | train_tok=seq_mean:3116 prompt_mean:2874 loss_mean:246 loss_max:246 | gpu_mem(GB)=alloc:9.49 res:15.71 max_alloc:23.60 max_res:38.98 | elapsed=426.9m\n",
      "20:11:16 | INFO    | [step 465] train_loss=0.6033 | lr=2.94943e-07 | grad_norm=6.2068 | train_tok=seq_mean:3123 prompt_mean:2837 loss_mean:291 loss_max:291 | gpu_mem(GB)=alloc:9.49 res:11.69 max_alloc:23.60 max_res:38.98 | elapsed=430.7m\n",
      "20:15:14 | INFO    | [step 470] train_loss=0.5604 | lr=2.86629e-07 | grad_norm=5.3452 | train_tok=seq_mean:2916 prompt_mean:2608 loss_mean:312 loss_max:312 | gpu_mem(GB)=alloc:9.49 res:22.10 max_alloc:23.60 max_res:38.98 | elapsed=434.6m\n",
      "20:19:07 | INFO    | [step 475] train_loss=0.6681 | lr=2.78368e-07 | grad_norm=6.5312 | train_tok=seq_mean:2717 prompt_mean:2423 loss_mean:297 loss_max:297 | gpu_mem(GB)=alloc:9.49 res:23.88 max_alloc:23.60 max_res:38.98 | elapsed=438.5m\n",
      "20:23:06 | INFO    | [step 480] train_loss=0.6769 | lr=2.70163e-07 | grad_norm=6.1869 | train_tok=seq_mean:3121 prompt_mean:2876 loss_mean:252 loss_max:252 | gpu_mem(GB)=alloc:9.49 res:24.40 max_alloc:23.60 max_res:38.98 | elapsed=442.5m\n",
      "20:27:06 | INFO    | [step 485] train_loss=0.6930 | lr=2.6202e-07 | grad_norm=8.0592 | train_tok=seq_mean:3115 prompt_mean:2876 loss_mean:244 loss_max:244 | gpu_mem(GB)=alloc:9.49 res:35.95 max_alloc:23.60 max_res:38.98 | elapsed=446.5m\n",
      "20:30:56 | INFO    | [step 490] train_loss=0.6894 | lr=2.5394e-07 | grad_norm=5.7861 | train_tok=seq_mean:3129 prompt_mean:2849 loss_mean:287 loss_max:287 | gpu_mem(GB)=alloc:9.49 res:28.37 max_alloc:23.60 max_res:38.98 | elapsed=450.3m\n",
      "20:34:35 | INFO    | [step 495] train_loss=0.7855 | lr=2.45929e-07 | grad_norm=7.9118 | train_tok=seq_mean:3104 prompt_mean:2839 loss_mean:265 loss_max:265 | gpu_mem(GB)=alloc:9.49 res:13.17 max_alloc:23.60 max_res:38.98 | elapsed=454.0m\n",
      "20:38:35 | INFO    | [step 500] train_loss=0.4915 | lr=2.3799e-07 | grad_norm=5.1087 | train_tok=seq_mean:2792 prompt_mean:2484 loss_mean:308 loss_max:308 | gpu_mem(GB)=alloc:9.49 res:24.56 max_alloc:23.60 max_res:38.98 | elapsed=458.0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 340\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:46:25 | INFO    | [step 500] eval_loss=0.0796 | train_tok=seq_mean:2576 prompt_mean:2284 loss_mean:292 loss_max:292 | gpu_mem(GB)=alloc:9.49 res:37.84 max_alloc:23.60 max_res:38.98 | elapsed=465.8m\n",
      "20:46:25 | INFO    | EarlyStop(metric=eval_loss): improved from 0.085384 to 0.079573\n",
      "20:50:04 | INFO    | [step 505] train_loss=0.5752 | lr=2.30126e-07 | grad_norm=7.0757 | train_tok=seq_mean:3165 prompt_mean:2885 loss_mean:283 loss_max:283 | gpu_mem(GB)=alloc:9.49 res:28.74 max_alloc:23.60 max_res:38.98 | elapsed=469.5m\n",
      "20:53:54 | INFO    | [step 510] train_loss=0.6461 | lr=2.22342e-07 | grad_norm=5.0928 | train_tok=seq_mean:3092 prompt_mean:2846 loss_mean:250 loss_max:250 | gpu_mem(GB)=alloc:9.49 res:18.21 max_alloc:23.60 max_res:38.98 | elapsed=473.3m\n",
      "20:57:45 | INFO    | [step 515] train_loss=0.5249 | lr=2.14641e-07 | grad_norm=5.5925 | train_tok=seq_mean:2771 prompt_mean:2462 loss_mean:314 loss_max:314 | gpu_mem(GB)=alloc:9.49 res:38.21 max_alloc:23.60 max_res:38.98 | elapsed=477.1m\n",
      "21:01:42 | INFO    | [step 520] train_loss=0.5379 | lr=2.07026e-07 | grad_norm=5.6855 | train_tok=seq_mean:2522 prompt_mean:2264 loss_mean:264 loss_max:264 | gpu_mem(GB)=alloc:9.49 res:17.55 max_alloc:23.60 max_res:38.98 | elapsed=481.1m\n",
      "21:05:34 | INFO    | [step 525] train_loss=0.7338 | lr=1.99501e-07 | grad_norm=5.5717 | train_tok=seq_mean:3123 prompt_mean:2835 loss_mean:293 loss_max:293 | gpu_mem(GB)=alloc:9.49 res:16.38 max_alloc:23.60 max_res:38.98 | elapsed=485.0m\n",
      "21:09:26 | INFO    | [step 530] train_loss=0.5429 | lr=1.9207e-07 | grad_norm=3.8661 | train_tok=seq_mean:2778 prompt_mean:2468 loss_mean:316 loss_max:316 | gpu_mem(GB)=alloc:9.49 res:15.18 max_alloc:23.60 max_res:38.98 | elapsed=488.8m\n",
      "21:13:20 | INFO    | [step 535] train_loss=0.5967 | lr=1.84736e-07 | grad_norm=4.1004 | train_tok=seq_mean:3124 prompt_mean:2875 loss_mean:253 loss_max:253 | gpu_mem(GB)=alloc:9.49 res:33.76 max_alloc:23.60 max_res:38.98 | elapsed=492.7m\n",
      "21:17:15 | INFO    | [step 540] train_loss=0.5087 | lr=1.77503e-07 | grad_norm=218.9787 | train_tok=seq_mean:3104 prompt_mean:2863 loss_mean:241 loss_max:241 | gpu_mem(GB)=alloc:9.49 res:26.24 max_alloc:23.60 max_res:38.98 | elapsed=496.6m\n",
      "21:21:03 | INFO    | [step 545] train_loss=0.5883 | lr=1.70373e-07 | grad_norm=6.6109 | train_tok=seq_mean:3075 prompt_mean:2832 loss_mean:248 loss_max:248 | gpu_mem(GB)=alloc:9.49 res:17.14 max_alloc:23.60 max_res:38.98 | elapsed=500.5m\n",
      "21:24:58 | INFO    | [step 550] train_loss=0.4583 | lr=1.6335e-07 | grad_norm=10.1232 | train_tok=seq_mean:2639 prompt_mean:2311 loss_mean:329 loss_max:329 | gpu_mem(GB)=alloc:9.49 res:24.58 max_alloc:23.60 max_res:38.98 | elapsed=504.4m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 340\n",
      "  Batch size = 1\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3 — Run fine-tune (усі параметри run_finetune + коментарі)\n",
    "# =========================\n",
    "\n",
    "# run_finetune(cfg, ...) — entrypoint, which:\n",
    "#   - load dadaset from prepared JSONL\n",
    "#   - Load tokenizer+model (QLoRA 4-bit) + LoRA\n",
    "#   - A/B BEFORE (optional)\n",
    "#   - Build trainer ( completion-only loss throgh  masking collator)\n",
    "#   - add callbacks \n",
    "#   - train\n",
    "#   - save lora_adapter + tokenizer\n",
    "#   - A/B AFTER + ab_report (optional)\n",
    "\n",
    "\n",
    "sft_artifacts = run_finetune(\n",
    "    cfg,\n",
    "    ab_indices = None,\n",
    "    # ab_indices = list(range(1)),# indexes from val_jsonl to use in A/B \"before/after\" (strict JSON parse rate )\n",
    "    # ab_indices=[0, 1, 10, 25, 50,  100, 150,  200, 250, 300,],   \n",
    "    do_ab_before=False,    #  True → will generate ab_before.json \n",
    "    do_ab_after=True,     #  True → will generate ab_after.json and make ab_report.md\n",
    "    dataset_limits=(None, None),  # (train_limit, val_limit) (500, 100)   # None → full dataset.\n",
    "    dataset_mode=\"prompt_completion\", # only it for now\n",
    "    clean_cuda_cache_before=True, #  True → before start will do: gc.collect() + torch.cuda.empty_cache()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67871c06-7365-4ba4-9d10-5fec26ab97dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sft_artifacts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msft_artifacts\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'sft_artifacts' is not defined"
     ]
    }
   ],
   "source": [
    "sft_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112393dc-33fe-4e07-b283-26372a091faa",
   "metadata": {},
   "source": [
    "# 🧠 Fine-tuning Mamay12B: SFT + DPO — Practical Guide\n",
    "#### QLoRA — Quantized Low-Rank Adaptation\n",
    "\n",
    "This guide describes **two training stages** (SFT → DPO) for a tariff recommendation assistant  \n",
    "and **what to monitor in metrics + which parameters to control**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ SFT — Supervised Fine-Tuning\n",
    "\n",
    "### 🎯 Goal\n",
    "- teach the model a **stable JSON format**\n",
    "- correct usage analysis\n",
    "- proper language and response structure  \n",
    "> **SFT does not optimize tariff selection**, only behavior and formatting.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Key metrics (monitor in logs)\n",
    "\n",
    "#### 🔹 `train_loss`\n",
    "- expected: **smoothly decreases**\n",
    "- typical range: `~1.5 → 0.5–0.8`\n",
    "- ❌ bad: sharp drop to `~0.0` → overfitting\n",
    "\n",
    "#### 🔹 `eval_loss`\n",
    "- should **correlate** with `train_loss`\n",
    "- ❌ if `train ↓` while `eval ↑` → overfitting\n",
    "\n",
    "#### 🔹 A/B sanity (before / after)\n",
    "(via `ABSanityCallback`)\n",
    "- JSON parses in **100% of cases**\n",
    "- all required fields are present\n",
    "- `tariffId ∈ avail_tp_with_desc`\n",
    "- text is in Ukrainian\n",
    "\n",
    "---\n",
    "\n",
    "### 🎛️ Main knobs (SFT)\n",
    "\n",
    "| Symptom | What to change |\n",
    "|------|----------|\n",
    "| loss does not decrease | ↑ `learning_rate` (5e-5 → 1e-4) |\n",
    "| fast overfitting | ↓ `learning_rate`, ↓ `num_train_epochs` |\n",
    "| brittle JSON | ↑ dataset size, ↓ LR |\n",
    "| slow training | ↓ `max_seq_len`, ↑ `grad_accum` |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6579717-7b9f-4b99-9db3-e8fecfe4808c",
   "metadata": {},
   "source": [
    "## SFT metrics: what they mean & how to tune them (Tariff Recommender)\n",
    "\n",
    "This notebook runs **SFT (Supervised Fine-Tuning)** with **completion-only loss**:\n",
    "- We feed a long `prompt` (facts + question + formatting rules)\n",
    "- We train the model to generate the `completion` (assistant JSON)\n",
    "- **Loss is computed only on completion tokens** (prompt tokens are masked out)\n",
    "\n",
    "SFT teaches:\n",
    "- stable output format (strict JSON)\n",
    "- the correct content structure (fields, Ukrainian text sections)\n",
    "- general mapping from usage → recommendation patterns (but not pairwise ranking like DPO)\n",
    "\n",
    "---\n",
    "\n",
    "### Key logged metrics (what they mean)\n",
    "\n",
    "#### 1) `train_loss`\n",
    "- Cross-entropy loss on **completion tokens only**.\n",
    "- Lower is better, but:\n",
    "  - very low train_loss can mean **overfitting** (especially on small datasets)\n",
    "  - always compare with `eval_loss`\n",
    "\n",
    "**If train_loss drops fast but eval_loss stalls or rises:**\n",
    "- overfitting → reduce steps/epochs, reduce LR, add regularization (weight_decay, dropout), or stop early\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) `eval_loss`\n",
    "- Cross-entropy loss on validation set completion tokens.\n",
    "- This is your primary “generalization” signal.\n",
    "\n",
    "Interpretation:\n",
    "- **decreasing** eval_loss → model generalizes better\n",
    "- **flat** eval_loss → you’re near the best point\n",
    "- **increasing** eval_loss → overfitting (stop / revert to best checkpoint)\n",
    "\n",
    "✅ Typical workflow:\n",
    "- Use early stopping on `eval_loss`\n",
    "- Keep the checkpoint with the **lowest** eval_loss\n",
    "\n",
    "---\n",
    "\n",
    "#### 3) `learning_rate` (LR schedule)\n",
    "- LR warms up and then follows your scheduler (cosine/linear).\n",
    "- LR that is too high can:\n",
    "  - make training unstable\n",
    "  - harm format stability (JSON breaks)\n",
    "- LR too low can:\n",
    "  - learn very slowly / plateau early\n",
    "\n",
    "**Recommended starting LR for your setup (12B + QLoRA + long prompts):**\n",
    "- `learning_rate = 5e-5` (`0.00005`)\n",
    "- avoid `1e-4` unless you have a large, diverse dataset and see stable format metrics\n",
    "\n",
    "---\n",
    "\n",
    "#### 4) `grad_norm`\n",
    "- How “big” the update is at each step (stability indicator).\n",
    "- Useful for catching too-aggressive training before quality degrades.\n",
    "\n",
    "Rules of thumb:\n",
    "- stable regime: `~1–10` (depends on setup)\n",
    "- frequent spikes `>15–20` → LR too high or batch noise → reduce LR / clip gradients\n",
    "\n",
    "---\n",
    "\n",
    "#### 5) Token stats: `train_tok=...`\n",
    "Example:\n",
    "- `seq_mean`: average total tokens fed to the model\n",
    "- `prompt_mean`: average prompt tokens (masked from loss)\n",
    "- `loss_mean`: average tokens contributing to loss (completion length)\n",
    "- `loss_max`: max completion length seen in the batch\n",
    "\n",
    "Why it matters:\n",
    "- if `loss_mean` becomes tiny (e.g., 20–50) you’re barely training on the answer\n",
    "- if `seq_mean` is close to `max_seq_len`, you risk truncation\n",
    "\n",
    "✅ In your case:\n",
    "- prompt ~2800–2900, completion ~250–350\n",
    "- ensure `max_seq_len=4096` to avoid cutting prompt or answer\n",
    "\n",
    "---\n",
    "\n",
    "#### 6) Format/quality sanity metric: `valid_json_rate` (from A/B sanity)\n",
    "- % of sanity examples where the generated output is valid JSON (and parseable).\n",
    "- This is **production-critical** for your tariff assistant.\n",
    "\n",
    "Interpretation:\n",
    "- `100%` → stable format\n",
    "- dips (e.g., `80%`) → likely decoding randomness or format instability\n",
    "\n",
    "✅ For reliable tracking, run sanity with deterministic generation:\n",
    "- `do_sample=False`, `num_beams=1`\n",
    "- explicitly disable sampling params (temperature/top_p/top_k)\n",
    "\n",
    "---\n",
    "\n",
    "### What to tune (control knobs)\n",
    "\n",
    "#### A) `learning_rate` (most important)\n",
    "Safe defaults for your setup:\n",
    "- `learning_rate = 5e-5` (`0.00005`)\n",
    "If you see instability / JSON breaks:\n",
    "- reduce to `3e-5` (`0.00003`)\n",
    "If you learn too slowly:\n",
    "- increase slightly, but prefer more steps over large LR\n",
    "\n",
    "Symptoms → Fix:\n",
    "- `grad_norm` spikes, `train_loss` noisy → lower LR\n",
    "- `valid_json_rate` drops → lower LR + deterministic eval + maybe reduce steps\n",
    "\n",
    "---\n",
    "\n",
    "#### B) `max_steps` / `num_train_epochs`\n",
    "SFT often converges quickly on structured outputs.\n",
    "Use either:\n",
    "- fixed `max_steps` (best for reproducibility)\n",
    "- or `num_train_epochs` (less predictable if dataset changes)\n",
    "\n",
    "For ~3k rows, `batch=1`, `grad_accum=8`:\n",
    "- 1 epoch ≈ ~`3094/8 ≈ 387` optimizer steps\n",
    "\n",
    "Practical:\n",
    "- start with `max_steps = 400–800` (≈ 1–2 epochs)\n",
    "- rely on early stopping to stop before overfitting\n",
    "\n",
    "---\n",
    "\n",
    "#### C) `eval_steps` (how often to evaluate)\n",
    "Trade-off:\n",
    "- frequent eval = better early stopping decisions but slower runs\n",
    "\n",
    "Practical:\n",
    "- `eval_steps = 100` (good default)\n",
    "- if eval is expensive, `eval_steps = 200` is ok (you used 200)\n",
    "\n",
    "---\n",
    "\n",
    "#### D) `save_steps` and `save_total_limit`\n",
    "Keep enough checkpoints to recover best eval point:\n",
    "- `save_steps = eval_steps` (common)\n",
    "- `save_total_limit = 2–3`\n",
    "\n",
    "---\n",
    "\n",
    "#### E) Regularization knobs\n",
    "Useful when eval_loss stops improving while train_loss keeps falling:\n",
    "- `weight_decay = 0.01` (you already added — good)\n",
    "- `lora_dropout = 0.05` (good)\n",
    "- reduce `lora_r` if needed (e.g., 16 → 8) for smaller capacity (rare)\n",
    "\n",
    "---\n",
    "\n",
    "#### F) Sequence length and truncation protection\n",
    "Given your distribution (prompt p95 ~2880, answers ~300–350):\n",
    "- use `max_seq_len = 4096`\n",
    "- keep `max_new_tokens_eval` around `256–512`\n",
    "\n",
    "Symptoms of truncation:\n",
    "- worse eval_loss\n",
    "- inconsistent content fields\n",
    "- model ignores the tail of `avail_tp_with_desc`\n",
    "\n",
    "---\n",
    "\n",
    "### What to watch for in practice (quick checklist)\n",
    "\n",
    "✅ Healthy SFT run:\n",
    "- `train_loss` decreases steadily\n",
    "- `eval_loss` decreases then plateaus\n",
    "- `valid_json_rate` stays high (ideally 100% on deterministic sanity)\n",
    "- generations remain consistent before/after checkpoints\n",
    "\n",
    "⚠️ Overfitting signs:\n",
    "- `train_loss` keeps dropping but `eval_loss` stops improving or rises\n",
    "- outputs become overly rigid, repetitive, or “memorized”\n",
    "- downstream DPO becomes too easy (margins explode fast)\n",
    "\n",
    "**Fix overfitting:**\n",
    "- fewer steps / fewer epochs\n",
    "- lower `learning_rate`\n",
    "- stronger regularization (weight_decay / dropout)\n",
    "- early stopping on `eval_loss`\n",
    "\n",
    "---\n",
    "\n",
    "### Recommended production evaluation pattern\n",
    "- **Full eval**: every `eval_steps` via `eval_loss`\n",
    "- **Sanity subset**: run A/B sanity on a fixed set of indices\n",
    "  - keep generation deterministic\n",
    "  - track:\n",
    "    - `valid_json_rate`\n",
    "    - presence/validity of required fields (`tariffId`, `templateId`, etc.)\n",
    "    - (optional) exact tariffId match rate on sanity subset\n",
    "\n",
    "This combination prevents “loss looks good but output is broken” failures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7987278-2f65-4ee1-9f57-515d4eaa9a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4a23d-ce64-4e23-91c3-b87e8de1f795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334bb68-22eb-4369-86a8-432d4ac158a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
