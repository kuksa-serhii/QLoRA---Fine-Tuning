{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b87b60-fe40-411d-86ac-ba09eb35cae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100-SXM4-40GB\n",
      "allocated: 9656.4775390625 MB\n",
      "reserved: 35986.0 MB\n",
      "allocated: 3875.18896484375 MB\n",
      "reserved: 3944.0 MB\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1 — ENV (MUST be first, before torch/transformers)\n",
    "# =========================\n",
    "from ft_pipeline.env import apply_env\n",
    "apply_env()\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n",
    "\n",
    "print(\"allocated:\", torch.cuda.memory_allocated()/1024**2, \"MB\")\n",
    "print(\"reserved:\",  torch.cuda.memory_reserved()/1024**2, \"MB\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"allocated:\", torch.cuda.memory_allocated()/1024**2, \"MB\")\n",
    "print(\"reserved:\",  torch.cuda.memory_reserved()/1024**2, \"MB\")\n",
    "\n",
    "\n",
    "import logging\n",
    "from ft_pipeline.logger import setup_logger\n",
    "from ft_pipeline.config import FTConfig\n",
    "from ft_pipeline.run_sft import run_finetune\n",
    "from ft_pipeline.config import DPOCfg\n",
    "from ft_pipeline.run_dpo import run_dpo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc98c0b5-88df-419b-850d-8fc420dafdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOCfg(model_id='/home/jovyan/ai-models/MamayLM-Gemma-3-12B', sft_adapter_dir='MamayLM-Gemma-3-12b_QLoRA_SFT/lora_adapter', dpo_train_jsonl='ft_datasets/dpo_train.jsonl', dpo_val_jsonl='ft_datasets/dpo_val.jsonl', out_dir='MamayLM-Gemma-3-12b_QLoRA_SFT_DPO', max_seq_len=5000, per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=8, learning_rate=5e-07, weight_decay=0.05, num_train_epochs=2.0, max_steps=None, warmup_ratio=0.05, lr_scheduler_type='cosine', logging_steps=5, eval_steps=50, save_steps=200, save_total_limit=2, beta=0.03, use_bf16=True, use_fp16=False, load_in_4bit=True, attn_implementation='sdpa', report_to='none', optim='paged_adamw_8bit', max_new_tokens_eval=512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Logger ft_pipeline (INFO)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell — DPO Config\n",
    "# =========================\n",
    "# DPO (Direct Preference Optimization) after SFT (QLoRA LoRA-adapter)\n",
    "# MamayLM-Gemma-3-12B (локальний шлях)\n",
    "# GPU: A100 40GB, BF16, QLoRA 4-bit\n",
    "\n",
    "from ft_pipeline.config import DPOCfg\n",
    "\n",
    "cfg_dpo = DPOCfg(\n",
    "    # ==========================================================\n",
    "    # BASE MODEL + CONTINUATION FROM SFT\n",
    "    # ==========================================================\n",
    "    model_id=\"/home/jovyan/ai-models/MamayLM-Gemma-3-12B\",               # path or HF repo id base model\n",
    "    \n",
    "    sft_adapter_dir=\"MamayLM-Gemma-3-12b_QLoRA_SFT/lora_adapter\",           # LoRA-adapter after SFT,\n",
    "    # sft_adapter_dir=None,                                              # if DPO from base\n",
    "    \n",
    "    dpo_train_jsonl=\"ft_datasets/dpo_train.jsonl\",                      # train dataset in JSONL\n",
    "    dpo_val_jsonl=\"ft_datasets/dpo_val.jsonl\",                          # validation dataset in JSONL\n",
    "\n",
    "    \n",
    "    # out_dir=\"outputs_mamay12b_qlora_dpo\",\n",
    "    out_dir=\"MamayLM-Gemma-3-12b_QLoRA_SFT_DPO\",\n",
    "    # out_dir=\"MamayLM-Gemma-3-12b_QLoRA_DPO\",\n",
    "    \n",
    "    # ==========================================================\n",
    "    # SEQUENCE / BATCHING\n",
    "    # ==========================================================\n",
    "    max_seq_len=5000,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # effective batch = batch_size * grad_accum\n",
    "\n",
    "    # ==========================================================\n",
    "    # TRAINING SCHEDULE / OPTIM\n",
    "    # ==========================================================\n",
    "    learning_rate=0.0000005,     \n",
    "    weight_decay=0.05,        # L2 regularization \n",
    "    num_train_epochs=2.0,     # (if  max_steps are provided - will ignored)\n",
    "    max_steps=None,            # use instead - num_train_epochs\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",  #scheduler: \"cosine\", \"linear\", ...\n",
    "    logging_steps=5,\n",
    "    eval_steps=50,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # ==========================================================\n",
    "    # DPO CORE\n",
    "    # ==========================================================\n",
    "    \n",
    "    beta=0.03,   # to avoid overfit - safe start. -- if   accuracy getting to fast to 1.0 - change it  → 0.03\n",
    "\n",
    "    use_bf16=True,            \n",
    "    use_fp16=False,\n",
    "    load_in_4bit=True,         # QLoRA (bitsandbytes 4-bit)\n",
    "    attn_implementation=\"sdpa\",   \n",
    "    optim=\"paged_adamw_8bit\",  # bitsandbytes to reduce the memory\n",
    "    report_to=\"none\",\n",
    "    max_new_tokens_eval=512,  #  in A/B sanity (before/after)\n",
    ")\n",
    "\n",
    "print(cfg_dpo)\n",
    "\n",
    "setup_logger(level=logging.INFO, log_file=f\"{cfg_dpo.out_dir}/ft_run_dpo.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e649ae66-d7fa-40e3-9248-c6af8c8976da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:49:15 | INFO    | === DPO RUN START ===\n",
      "08:49:15 | INFO    | CUDA available=True\n",
      "08:49:15 | INFO    | CUDA device=NVIDIA A100-SXM4-40GB\n",
      "08:49:15 | INFO    | Loading tokenizer: /home/jovyan/ai-models/MamayLM-Gemma-3-12B\n",
      "08:49:16 | INFO    | Tokenizer loaded\n",
      "08:49:16 | INFO    | Loading DPO datasets\n",
      "08:49:16 | INFO    |   train: ft_datasets/dpo_train.jsonl\n",
      "08:49:16 | INFO    |   val:   ft_datasets/dpo_val.jsonl\n",
      "08:49:41 | INFO    | DPO dataset ready | train=3094 | val=344\n",
      "08:49:41 | INFO    | Loading base model (QLoRA)\n",
      "08:49:41 | INFO    |   model_id: /home/jovyan/ai-models/MamayLM-Gemma-3-12B\n",
      "08:49:41 | INFO    |   dtype: torch.bfloat16\n",
      "08:49:41 | INFO    |   4bit: True\n",
      "08:49:41 | INFO    |   attn_implementation: sdpa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737d7c2390114d2e99c2a27ff1312410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:51:06 | INFO    | Base model loaded\n",
      "08:51:06 | INFO    | Enabling gradient checkpointing\n",
      "08:51:06 | INFO    | Loading trainable LoRA adapter from: MamayLM-Gemma-3-12b_QLoRA_SFT/lora_adapter\n",
      "08:51:08 | INFO    | Trainable adapter loaded\n",
      "08:51:08 | INFO    | Enabled input require grads for gradient checkpointing\n",
      "08:51:08 | INFO    | Trainable parameters:\n",
      "trainable params: 68,456,448 || all params: 12,255,781,488 || trainable%: 0.5586\n",
      "08:51:08 | INFO    | Building DPOConfig\n",
      "08:51:08 | INFO    |   max_seq_len=5000\n",
      "08:51:08 | INFO    |   beta=0.03\n",
      "08:51:08 | INFO    |   lr=5e-07\n",
      "08:51:08 | INFO    | Building DPOTrainer\n",
      "08:51:08 | INFO    |   train_samples=3094\n",
      "08:51:08 | INFO    |   val_samples=344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:51:09 | INFO    | Starting DPO training…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot get num_tokens from dataloader\n",
      "skipped Embedding(4096, 1152): 4.5M params\n",
      "skipped Gemma3TextScaledWordEmbedding(262208, 3840, padding_idx=0): 964.734375M params\n",
      "skipped: 964.734375M params\n",
      "***** Running training *****\n",
      "  Num examples = 3,094\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 774\n",
      "  Number of trainable parameters = 68,456,448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:51:10 | INFO    | GPUMetricsCallback enabled\n",
      "08:51:10 | INFO    | DPOMetricsCallback enabled | csv=MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/dpo_metrics.csv | every_n_steps=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='774' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/774 15:07:24 < 10:56:15, 0.01 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.496200</td>\n",
       "      <td>3.469268</td>\n",
       "      <td>-0.057585</td>\n",
       "      <td>3.376739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.434324</td>\n",
       "      <td>-267.320862</td>\n",
       "      <td>-67.623283</td>\n",
       "      <td>-2.042471</td>\n",
       "      <td>-3.212824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.327000</td>\n",
       "      <td>2.257684</td>\n",
       "      <td>1.033092</td>\n",
       "      <td>3.169737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.136645</td>\n",
       "      <td>-230.964935</td>\n",
       "      <td>-74.523338</td>\n",
       "      <td>-1.982634</td>\n",
       "      <td>-2.661982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.408200</td>\n",
       "      <td>1.290876</td>\n",
       "      <td>1.844689</td>\n",
       "      <td>2.784632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.939943</td>\n",
       "      <td>-203.911697</td>\n",
       "      <td>-87.360168</td>\n",
       "      <td>-1.971564</td>\n",
       "      <td>-2.398803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.601600</td>\n",
       "      <td>0.564815</td>\n",
       "      <td>2.313618</td>\n",
       "      <td>1.963990</td>\n",
       "      <td>0.735465</td>\n",
       "      <td>0.349628</td>\n",
       "      <td>-188.280746</td>\n",
       "      <td>-114.714920</td>\n",
       "      <td>-2.000070</td>\n",
       "      <td>-2.305380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.162448</td>\n",
       "      <td>2.760528</td>\n",
       "      <td>0.881279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.879249</td>\n",
       "      <td>-173.383743</td>\n",
       "      <td>-150.805283</td>\n",
       "      <td>-2.046856</td>\n",
       "      <td>-2.270607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.040745</td>\n",
       "      <td>3.244718</td>\n",
       "      <td>-0.083170</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.327888</td>\n",
       "      <td>-157.244080</td>\n",
       "      <td>-182.953583</td>\n",
       "      <td>-2.094893</td>\n",
       "      <td>-2.238698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.014001</td>\n",
       "      <td>3.533666</td>\n",
       "      <td>-0.858690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.392356</td>\n",
       "      <td>-147.612488</td>\n",
       "      <td>-208.804276</td>\n",
       "      <td>-2.161052</td>\n",
       "      <td>-2.233387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>3.687239</td>\n",
       "      <td>-1.333563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.020802</td>\n",
       "      <td>-142.493362</td>\n",
       "      <td>-224.633331</td>\n",
       "      <td>-2.226531</td>\n",
       "      <td>-2.247115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>3.749401</td>\n",
       "      <td>-1.622471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.371871</td>\n",
       "      <td>-140.421310</td>\n",
       "      <td>-234.263626</td>\n",
       "      <td>-2.277489</td>\n",
       "      <td>-2.263341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:52:44 | INFO    | [step 1] train_loss=4.3627 | lr=0 | grad_norm=40.3988 | gpu_mem(GB)=alloc:9.49 res:23.14 max_alloc:36.02 max_res:38.98 | elapsed=1.6m\n",
      "08:58:53 | INFO    | [step 5] train_loss=4.4773 | lr=5.12821e-08 | grad_norm=37.3110 | gpu_mem(GB)=alloc:9.49 res:27.82 max_alloc:36.24 max_res:38.98 | elapsed=7.7m\n",
      "08:58:53 | INFO    | DPO step=5 | loss=4.4773 | rewards/accuracies=0.0000 | rewards/margins=-4.4647 | rewards/chosen=-1.0012 | rewards/rejected=3.4635\n",
      "09:06:33 | INFO    | [step 10] train_loss=4.4379 | lr=1.15385e-07 | grad_norm=36.2712 | gpu_mem(GB)=alloc:9.49 res:27.02 max_alloc:36.28 max_res:38.98 | elapsed=15.4m\n",
      "09:06:33 | INFO    | DPO step=10 | loss=4.4379 | rewards/accuracies=0.0000 | rewards/margins=-4.4248 | rewards/chosen=-0.9306 | rewards/rejected=3.4942\n",
      "09:14:20 | INFO    | [step 15] train_loss=4.4845 | lr=1.79487e-07 | grad_norm=33.4385 | gpu_mem(GB)=alloc:9.49 res:29.11 max_alloc:36.28 max_res:38.98 | elapsed=23.2m\n",
      "09:14:20 | INFO    | DPO step=15 | loss=4.4845 | rewards/accuracies=0.0000 | rewards/margins=-4.4720 | rewards/chosen=-0.9670 | rewards/rejected=3.5050\n",
      "09:22:01 | INFO    | [step 20] train_loss=4.3490 | lr=2.4359e-07 | grad_norm=106.8321 | gpu_mem(GB)=alloc:9.49 res:16.87 max_alloc:36.28 max_res:38.98 | elapsed=30.9m\n",
      "09:22:01 | INFO    | DPO step=20 | loss=4.3490 | rewards/accuracies=0.0000 | rewards/margins=-4.3345 | rewards/chosen=-0.8786 | rewards/rejected=3.4559\n",
      "09:29:34 | INFO    | [step 25] train_loss=4.2273 | lr=3.07692e-07 | grad_norm=39.2720 | gpu_mem(GB)=alloc:9.49 res:35.19 max_alloc:36.28 max_res:38.98 | elapsed=38.4m\n",
      "09:29:34 | INFO    | DPO step=25 | loss=4.2273 | rewards/accuracies=0.0000 | rewards/margins=-4.2111 | rewards/chosen=-0.7781 | rewards/rejected=3.4330\n",
      "09:37:13 | INFO    | [step 30] train_loss=4.1196 | lr=3.71795e-07 | grad_norm=32.8993 | gpu_mem(GB)=alloc:9.49 res:33.82 max_alloc:36.28 max_res:38.98 | elapsed=46.1m\n",
      "09:37:13 | INFO    | DPO step=30 | loss=4.1196 | rewards/accuracies=0.0000 | rewards/margins=-4.1020 | rewards/chosen=-0.7078 | rewards/rejected=3.3942\n",
      "09:44:59 | INFO    | [step 35] train_loss=4.0180 | lr=4.35897e-07 | grad_norm=31.7732 | gpu_mem(GB)=alloc:9.49 res:26.87 max_alloc:36.49 max_res:38.98 | elapsed=53.8m\n",
      "09:44:59 | INFO    | DPO step=35 | loss=4.0180 | rewards/accuracies=0.0000 | rewards/margins=-3.9983 | rewards/chosen=-0.5304 | rewards/rejected=3.4679\n",
      "09:52:39 | INFO    | [step 40] train_loss=3.8194 | lr=5e-07 | grad_norm=40.0236 | gpu_mem(GB)=alloc:9.49 res:29.03 max_alloc:36.49 max_res:38.98 | elapsed=61.5m\n",
      "09:52:39 | INFO    | DPO step=40 | loss=3.8194 | rewards/accuracies=0.0000 | rewards/margins=-3.7943 | rewards/chosen=-0.4422 | rewards/rejected=3.3521\n",
      "10:00:11 | INFO    | [step 45] train_loss=3.6602 | lr=4.99943e-07 | grad_norm=26.7964 | gpu_mem(GB)=alloc:9.49 res:20.89 max_alloc:36.49 max_res:38.98 | elapsed=69.0m\n",
      "10:00:11 | INFO    | DPO step=45 | loss=3.6602 | rewards/accuracies=0.0000 | rewards/margins=-3.6317 | rewards/chosen=-0.2973 | rewards/rejected=3.3343\n",
      "10:08:06 | INFO    | [step 50] train_loss=3.4962 | lr=4.99772e-07 | grad_norm=26.9188 | gpu_mem(GB)=alloc:9.49 res:18.54 max_alloc:36.53 max_res:38.98 | elapsed=76.9m\n",
      "10:08:06 | INFO    | DPO step=50 | loss=3.4962 | rewards/accuracies=0.0000 | rewards/margins=-3.4627 | rewards/chosen=-0.1285 | rewards/rejected=3.3342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 344\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:32:16 | INFO    | [step 50] eval_loss=3.4693 | gpu_mem(GB)=alloc:9.49 res:35.14 max_alloc:36.53 max_res:38.98 | elapsed=101.1m\n",
      "10:32:16 | INFO    | DPO[EVAL] step=50\n",
      "10:32:16 | INFO    | EarlyStop(metric=eval_loss): improved from None to 3.469268\n",
      "10:40:01 | INFO    | [step 55] train_loss=3.4836 | lr=4.99486e-07 | grad_norm=26.5273 | gpu_mem(GB)=alloc:9.49 res:19.97 max_alloc:36.53 max_res:38.98 | elapsed=108.8m\n",
      "10:40:01 | INFO    | DPO step=55 | loss=3.4836 | rewards/accuracies=0.0000 | rewards/margins=-3.4477 | rewards/chosen=-0.0047 | rewards/rejected=3.4429\n",
      "10:47:40 | INFO    | [step 60] train_loss=3.2114 | lr=4.99087e-07 | grad_norm=24.8024 | gpu_mem(GB)=alloc:9.49 res:22.39 max_alloc:36.53 max_res:38.98 | elapsed=116.5m\n",
      "10:47:40 | INFO    | DPO step=60 | loss=3.2114 | rewards/accuracies=0.0000 | rewards/margins=-3.1669 | rewards/chosen=0.1114 | rewards/rejected=3.2783\n",
      "10:55:22 | INFO    | [step 65] train_loss=3.2018 | lr=4.98574e-07 | grad_norm=23.6171 | gpu_mem(GB)=alloc:9.49 res:22.41 max_alloc:36.53 max_res:38.98 | elapsed=124.2m\n",
      "10:55:22 | INFO    | DPO step=65 | loss=3.2018 | rewards/accuracies=0.0000 | rewards/margins=-3.1552 | rewards/chosen=0.2277 | rewards/rejected=3.3829\n",
      "11:03:02 | INFO    | [step 70] train_loss=3.1272 | lr=4.97948e-07 | grad_norm=23.9902 | gpu_mem(GB)=alloc:9.49 res:37.65 max_alloc:36.53 max_res:38.98 | elapsed=131.9m\n",
      "11:03:02 | INFO    | DPO step=70 | loss=3.1272 | rewards/accuracies=0.0000 | rewards/margins=-3.0783 | rewards/chosen=0.3474 | rewards/rejected=3.4257\n",
      "11:10:38 | INFO    | [step 75] train_loss=2.8805 | lr=4.97208e-07 | grad_norm=26.8250 | gpu_mem(GB)=alloc:9.49 res:28.60 max_alloc:36.53 max_res:38.98 | elapsed=139.5m\n",
      "11:10:38 | INFO    | DPO step=75 | loss=2.8805 | rewards/accuracies=0.0000 | rewards/margins=-2.8177 | rewards/chosen=0.4640 | rewards/rejected=3.2817\n",
      "11:18:19 | INFO    | [step 80] train_loss=2.9092 | lr=4.96355e-07 | grad_norm=30.5213 | gpu_mem(GB)=alloc:9.49 res:38.84 max_alloc:36.53 max_res:38.98 | elapsed=147.2m\n",
      "11:18:19 | INFO    | DPO step=80 | loss=2.9092 | rewards/accuracies=0.0000 | rewards/margins=-2.8488 | rewards/chosen=0.5567 | rewards/rejected=3.4055\n",
      "11:26:00 | INFO    | [step 85] train_loss=2.6720 | lr=4.9539e-07 | grad_norm=23.7713 | gpu_mem(GB)=alloc:9.49 res:18.86 max_alloc:36.53 max_res:38.98 | elapsed=154.8m\n",
      "11:26:00 | INFO    | DPO step=85 | loss=2.6720 | rewards/accuracies=0.0000 | rewards/margins=-2.5927 | rewards/chosen=0.6568 | rewards/rejected=3.2494\n",
      "11:33:35 | INFO    | [step 90] train_loss=2.5050 | lr=4.94312e-07 | grad_norm=22.4171 | gpu_mem(GB)=alloc:9.49 res:12.49 max_alloc:36.53 max_res:38.98 | elapsed=162.4m\n",
      "11:33:35 | INFO    | DPO step=90 | loss=2.5050 | rewards/accuracies=0.0000 | rewards/margins=-2.4146 | rewards/chosen=0.7583 | rewards/rejected=3.1729\n",
      "11:41:14 | INFO    | [step 95] train_loss=2.5462 | lr=4.93124e-07 | grad_norm=21.4186 | gpu_mem(GB)=alloc:9.49 res:33.10 max_alloc:36.53 max_res:38.98 | elapsed=170.1m\n",
      "11:41:14 | INFO    | DPO step=95 | loss=2.5462 | rewards/accuracies=0.0000 | rewards/margins=-2.4549 | rewards/chosen=0.8754 | rewards/rejected=3.3303\n",
      "11:48:56 | INFO    | [step 100] train_loss=2.3270 | lr=4.91824e-07 | grad_norm=24.0552 | gpu_mem(GB)=alloc:9.49 res:30.50 max_alloc:36.53 max_res:38.98 | elapsed=177.8m\n",
      "11:48:56 | INFO    | DPO step=100 | loss=2.3270 | rewards/accuracies=0.0000 | rewards/margins=-2.2148 | rewards/chosen=0.9773 | rewards/rejected=3.1921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 344\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:13:08 | INFO    | [step 100] eval_loss=2.2577 | gpu_mem(GB)=alloc:9.49 res:35.14 max_alloc:36.53 max_res:38.98 | elapsed=202.0m\n",
      "12:13:08 | INFO    | DPO[EVAL] step=100\n",
      "12:13:08 | INFO    | EarlyStop(metric=eval_loss): improved from 3.469268 to 2.257684\n",
      "12:20:43 | INFO    | [step 105] train_loss=2.2072 | lr=4.90413e-07 | grad_norm=20.6044 | gpu_mem(GB)=alloc:9.49 res:30.32 max_alloc:36.53 max_res:38.98 | elapsed=209.6m\n",
      "12:20:43 | INFO    | DPO step=105 | loss=2.2072 | rewards/accuracies=0.0000 | rewards/margins=-2.0822 | rewards/chosen=1.0467 | rewards/rejected=3.1289\n",
      "12:28:23 | INFO    | [step 110] train_loss=2.1258 | lr=4.88893e-07 | grad_norm=19.1288 | gpu_mem(GB)=alloc:9.49 res:21.49 max_alloc:36.53 max_res:38.98 | elapsed=217.2m\n",
      "12:28:23 | INFO    | DPO step=110 | loss=2.1258 | rewards/accuracies=0.0000 | rewards/margins=-1.9860 | rewards/chosen=1.1575 | rewards/rejected=3.1435\n",
      "12:35:57 | INFO    | [step 115] train_loss=2.0704 | lr=4.87264e-07 | grad_norm=19.0702 | gpu_mem(GB)=alloc:9.49 res:25.91 max_alloc:36.53 max_res:38.98 | elapsed=224.8m\n",
      "12:35:57 | INFO    | DPO step=115 | loss=2.0704 | rewards/accuracies=0.0000 | rewards/margins=-1.9229 | rewards/chosen=1.2280 | rewards/rejected=3.1510\n",
      "12:43:38 | INFO    | [step 120] train_loss=1.9662 | lr=4.85526e-07 | grad_norm=41.9716 | gpu_mem(GB)=alloc:9.49 res:34.13 max_alloc:36.53 max_res:38.98 | elapsed=232.5m\n",
      "12:43:38 | INFO    | DPO step=120 | loss=1.9662 | rewards/accuracies=0.0000 | rewards/margins=-1.8013 | rewards/chosen=1.3146 | rewards/rejected=3.1159\n",
      "12:51:17 | INFO    | [step 125] train_loss=1.8451 | lr=4.83681e-07 | grad_norm=17.7605 | gpu_mem(GB)=alloc:9.49 res:37.59 max_alloc:36.53 max_res:38.98 | elapsed=240.1m\n",
      "12:51:17 | INFO    | DPO step=125 | loss=1.8451 | rewards/accuracies=0.0000 | rewards/margins=-1.6612 | rewards/chosen=1.3916 | rewards/rejected=3.0528\n",
      "12:59:07 | INFO    | [step 130] train_loss=1.7545 | lr=4.81729e-07 | grad_norm=17.5667 | gpu_mem(GB)=alloc:9.49 res:26.16 max_alloc:36.53 max_res:38.98 | elapsed=248.0m\n",
      "12:59:07 | INFO    | DPO step=130 | loss=1.7545 | rewards/accuracies=0.0000 | rewards/margins=-1.5512 | rewards/chosen=1.4740 | rewards/rejected=3.0252\n",
      "13:06:50 | INFO    | [step 135] train_loss=1.6499 | lr=4.79671e-07 | grad_norm=17.2115 | gpu_mem(GB)=alloc:9.49 res:21.05 max_alloc:36.53 max_res:38.98 | elapsed=255.7m\n",
      "13:06:50 | INFO    | DPO step=135 | loss=1.6499 | rewards/accuracies=0.0000 | rewards/margins=-1.4182 | rewards/chosen=1.5513 | rewards/rejected=2.9695\n",
      "13:14:31 | INFO    | [step 140] train_loss=1.5899 | lr=4.77509e-07 | grad_norm=18.0980 | gpu_mem(GB)=alloc:9.49 res:25.81 max_alloc:36.53 max_res:38.98 | elapsed=263.3m\n",
      "13:14:31 | INFO    | DPO step=140 | loss=1.5899 | rewards/accuracies=0.0000 | rewards/margins=-1.3373 | rewards/chosen=1.6177 | rewards/rejected=2.9550\n",
      "13:22:10 | INFO    | [step 145] train_loss=1.3369 | lr=4.75242e-07 | grad_norm=15.4631 | gpu_mem(GB)=alloc:9.49 res:26.06 max_alloc:36.53 max_res:38.98 | elapsed=271.0m\n",
      "13:22:10 | INFO    | DPO step=145 | loss=1.3369 | rewards/accuracies=0.0000 | rewards/margins=-1.0030 | rewards/chosen=1.7612 | rewards/rejected=2.7642\n",
      "13:29:48 | INFO    | [step 150] train_loss=1.4082 | lr=4.72873e-07 | grad_norm=18.8370 | gpu_mem(GB)=alloc:9.49 res:22.86 max_alloc:36.53 max_res:38.98 | elapsed=278.6m\n",
      "13:29:48 | INFO    | DPO step=150 | loss=1.4082 | rewards/accuracies=0.0000 | rewards/margins=-1.1002 | rewards/chosen=1.8121 | rewards/rejected=2.9123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 344\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:53:57 | INFO    | [step 150] eval_loss=1.2909 | gpu_mem(GB)=alloc:9.49 res:35.14 max_alloc:36.53 max_res:38.98 | elapsed=302.8m\n",
      "13:53:57 | INFO    | DPO[EVAL] step=150\n",
      "13:53:57 | INFO    | EarlyStop(metric=eval_loss): improved from 2.257684 to 1.290876\n",
      "14:01:46 | INFO    | [step 155] train_loss=1.2359 | lr=4.70402e-07 | grad_norm=14.8002 | gpu_mem(GB)=alloc:9.49 res:23.31 max_alloc:36.53 max_res:38.98 | elapsed=310.6m\n",
      "14:01:46 | INFO    | DPO step=155 | loss=1.2359 | rewards/accuracies=0.0000 | rewards/margins=-0.8620 | rewards/chosen=1.8655 | rewards/rejected=2.7275\n",
      "14:09:24 | INFO    | [step 160] train_loss=1.2100 | lr=4.6783e-07 | grad_norm=16.7697 | gpu_mem(GB)=alloc:9.49 res:26.65 max_alloc:36.53 max_res:38.98 | elapsed=318.2m\n",
      "14:09:24 | INFO    | DPO step=160 | loss=1.2100 | rewards/accuracies=0.0000 | rewards/margins=-0.8260 | rewards/chosen=1.9254 | rewards/rejected=2.7514\n",
      "14:17:00 | INFO    | [step 165] train_loss=1.1466 | lr=4.65158e-07 | grad_norm=14.2159 | gpu_mem(GB)=alloc:9.49 res:21.93 max_alloc:36.53 max_res:38.98 | elapsed=325.8m\n",
      "14:17:00 | INFO    | DPO step=165 | loss=1.1466 | rewards/accuracies=0.0000 | rewards/margins=-0.7305 | rewards/chosen=1.9805 | rewards/rejected=2.7110\n",
      "14:24:40 | INFO    | [step 170] train_loss=0.9805 | lr=4.62389e-07 | grad_norm=15.6750 | gpu_mem(GB)=alloc:9.49 res:25.75 max_alloc:36.53 max_res:38.98 | elapsed=333.5m\n",
      "14:24:40 | INFO    | DPO step=170 | loss=0.9805 | rewards/accuracies=0.0750 | rewards/margins=-0.4841 | rewards/chosen=2.0142 | rewards/rejected=2.4983\n",
      "14:32:13 | INFO    | [step 175] train_loss=1.0330 | lr=4.59522e-07 | grad_norm=14.3158 | gpu_mem(GB)=alloc:9.49 res:37.20 max_alloc:36.53 max_res:38.98 | elapsed=341.1m\n",
      "14:32:13 | INFO    | DPO step=175 | loss=1.0330 | rewards/accuracies=0.1750 | rewards/margins=-0.5443 | rewards/chosen=2.0892 | rewards/rejected=2.6335\n",
      "14:39:52 | INFO    | [step 180] train_loss=0.8954 | lr=4.5656e-07 | grad_norm=14.1181 | gpu_mem(GB)=alloc:9.49 res:23.43 max_alloc:36.53 max_res:38.98 | elapsed=348.7m\n",
      "14:39:52 | INFO    | DPO step=180 | loss=0.8954 | rewards/accuracies=0.4250 | rewards/margins=-0.3084 | rewards/chosen=2.1547 | rewards/rejected=2.4631\n",
      "14:47:29 | INFO    | [step 185] train_loss=0.7885 | lr=4.53503e-07 | grad_norm=10.3841 | gpu_mem(GB)=alloc:9.49 res:24.52 max_alloc:36.53 max_res:38.98 | elapsed=356.3m\n",
      "14:47:29 | INFO    | DPO step=185 | loss=0.7885 | rewards/accuracies=0.4750 | rewards/margins=-0.1351 | rewards/chosen=2.1912 | rewards/rejected=2.3264\n",
      "14:55:02 | INFO    | [step 190] train_loss=0.6965 | lr=4.50353e-07 | grad_norm=11.3080 | gpu_mem(GB)=alloc:9.49 res:24.38 max_alloc:36.53 max_res:38.98 | elapsed=363.9m\n",
      "14:55:02 | INFO    | DPO step=190 | loss=0.6965 | rewards/accuracies=0.5500 | rewards/margins=0.0411 | rewards/chosen=2.2407 | rewards/rejected=2.1996\n",
      "15:02:40 | INFO    | [step 195] train_loss=0.6724 | lr=4.47112e-07 | grad_norm=9.3433 | gpu_mem(GB)=alloc:9.49 res:28.18 max_alloc:36.53 max_res:38.98 | elapsed=371.5m\n",
      "15:02:40 | INFO    | DPO step=195 | loss=0.6724 | rewards/accuracies=0.6500 | rewards/margins=0.0915 | rewards/chosen=2.2672 | rewards/rejected=2.1757\n",
      "15:10:18 | INFO    | [step 200] train_loss=0.6016 | lr=4.43781e-07 | grad_norm=11.7746 | gpu_mem(GB)=alloc:9.49 res:15.60 max_alloc:36.53 max_res:38.98 | elapsed=379.1m\n",
      "15:10:18 | INFO    | DPO step=200 | loss=0.6016 | rewards/accuracies=0.7000 | rewards/margins=0.2495 | rewards/chosen=2.2912 | rewards/rejected=2.0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 344\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:34:24 | INFO    | [step 200] eval_loss=0.5648 | gpu_mem(GB)=alloc:9.49 res:35.14 max_alloc:36.53 max_res:38.98 | elapsed=403.2m\n",
      "15:34:24 | INFO    | DPO[EVAL] step=200\n",
      "15:34:24 | INFO    | EarlyStop(metric=eval_loss): improved from 1.290876 to 0.564815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/checkpoint-200\n",
      "chat template saved in MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/checkpoint-200/chat_template.jinja\n",
      "tokenizer config file saved in MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/checkpoint-200/special_tokens_map.json\n",
      "/opt/conda/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:42:04 | INFO    | [step 205] train_loss=0.5298 | lr=4.40361e-07 | grad_norm=8.6214 | gpu_mem(GB)=alloc:9.49 res:26.03 max_alloc:36.53 max_res:38.98 | elapsed=410.9m\n",
      "15:42:04 | INFO    | DPO step=205 | loss=0.5298 | rewards/accuracies=0.8750 | rewards/margins=0.4180 | rewards/chosen=2.3074 | rewards/rejected=1.8894\n",
      "15:49:44 | INFO    | [step 210] train_loss=0.4096 | lr=4.36855e-07 | grad_norm=6.3665 | gpu_mem(GB)=alloc:9.49 res:34.04 max_alloc:36.53 max_res:38.98 | elapsed=418.6m\n",
      "15:49:44 | INFO    | DPO step=210 | loss=0.4096 | rewards/accuracies=0.9250 | rewards/margins=0.7402 | rewards/chosen=2.3274 | rewards/rejected=1.5872\n",
      "15:57:25 | INFO    | [step 215] train_loss=0.4846 | lr=4.33263e-07 | grad_norm=9.4812 | gpu_mem(GB)=alloc:9.49 res:35.27 max_alloc:36.53 max_res:38.98 | elapsed=426.2m\n",
      "15:57:25 | INFO    | DPO step=215 | loss=0.4846 | rewards/accuracies=0.8750 | rewards/margins=0.5695 | rewards/chosen=2.3454 | rewards/rejected=1.7759\n",
      "16:05:03 | INFO    | [step 220] train_loss=0.4103 | lr=4.29587e-07 | grad_norm=7.9736 | gpu_mem(GB)=alloc:9.49 res:37.22 max_alloc:36.53 max_res:38.98 | elapsed=433.9m\n",
      "16:05:03 | INFO    | DPO step=220 | loss=0.4103 | rewards/accuracies=0.9250 | rewards/margins=0.7728 | rewards/chosen=2.3759 | rewards/rejected=1.6031\n",
      "16:15:10 | INFO    | [step 225] train_loss=0.3379 | lr=4.2583e-07 | grad_norm=6.9790 | gpu_mem(GB)=alloc:9.49 res:34.07 max_alloc:36.53 max_res:38.98 | elapsed=444.0m\n",
      "16:15:10 | INFO    | DPO step=225 | loss=0.3379 | rewards/accuracies=0.9750 | rewards/margins=0.9924 | rewards/chosen=2.4238 | rewards/rejected=1.4314\n",
      "16:23:18 | INFO    | [step 230] train_loss=0.3265 | lr=4.21992e-07 | grad_norm=9.1476 | gpu_mem(GB)=alloc:9.49 res:28.37 max_alloc:36.53 max_res:38.98 | elapsed=452.1m\n",
      "16:23:18 | INFO    | DPO step=230 | loss=0.3265 | rewards/accuracies=1.0000 | rewards/margins=1.0671 | rewards/chosen=2.4732 | rewards/rejected=1.4060\n",
      "16:31:01 | INFO    | [step 235] train_loss=0.2589 | lr=4.18075e-07 | grad_norm=5.8108 | gpu_mem(GB)=alloc:9.49 res:21.83 max_alloc:36.53 max_res:38.98 | elapsed=459.8m\n",
      "16:31:01 | INFO    | DPO step=235 | loss=0.2589 | rewards/accuracies=1.0000 | rewards/margins=1.3285 | rewards/chosen=2.5269 | rewards/rejected=1.1983\n",
      "16:38:38 | INFO    | [step 240] train_loss=0.2016 | lr=4.14082e-07 | grad_norm=4.6601 | gpu_mem(GB)=alloc:9.49 res:33.13 max_alloc:36.53 max_res:38.98 | elapsed=467.5m\n",
      "16:38:38 | INFO    | DPO step=240 | loss=0.2016 | rewards/accuracies=1.0000 | rewards/margins=1.5822 | rewards/chosen=2.5559 | rewards/rejected=0.9737\n",
      "16:46:22 | INFO    | [step 245] train_loss=0.1828 | lr=4.10014e-07 | grad_norm=3.8439 | gpu_mem(GB)=alloc:9.49 res:36.87 max_alloc:36.53 max_res:38.98 | elapsed=475.2m\n",
      "16:46:22 | INFO    | DPO step=245 | loss=0.1828 | rewards/accuracies=1.0000 | rewards/margins=1.7245 | rewards/chosen=2.6476 | rewards/rejected=0.9231\n",
      "16:53:55 | INFO    | [step 250] train_loss=0.1851 | lr=4.05872e-07 | grad_norm=3.6123 | gpu_mem(GB)=alloc:9.49 res:30.63 max_alloc:36.53 max_res:38.98 | elapsed=482.8m\n",
      "16:53:55 | INFO    | DPO step=250 | loss=0.1851 | rewards/accuracies=1.0000 | rewards/margins=1.6808 | rewards/chosen=2.7095 | rewards/rejected=1.0287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 344\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:18:03 | INFO    | [step 250] eval_loss=0.1624 | gpu_mem(GB)=alloc:9.49 res:35.14 max_alloc:36.53 max_res:38.98 | elapsed=506.9m\n",
      "17:18:03 | INFO    | DPO[EVAL] step=250\n",
      "17:18:03 | INFO    | EarlyStop(metric=eval_loss): improved from 0.564815 to 0.162448\n",
      "17:25:52 | INFO    | [step 255] train_loss=0.1576 | lr=4.0166e-07 | grad_norm=3.4395 | gpu_mem(GB)=alloc:9.49 res:25.79 max_alloc:36.58 max_res:38.98 | elapsed=514.7m\n",
      "17:25:52 | INFO    | DPO step=255 | loss=0.1576 | rewards/accuracies=1.0000 | rewards/margins=1.8776 | rewards/chosen=2.7297 | rewards/rejected=0.8521\n",
      "17:33:34 | INFO    | [step 260] train_loss=0.1399 | lr=3.97378e-07 | grad_norm=4.2901 | gpu_mem(GB)=alloc:9.49 res:25.60 max_alloc:36.58 max_res:38.98 | elapsed=522.4m\n",
      "17:33:34 | INFO    | DPO step=260 | loss=0.1399 | rewards/accuracies=1.0000 | rewards/margins=2.0035 | rewards/chosen=2.8217 | rewards/rejected=0.8182\n",
      "17:41:09 | INFO    | [step 265] train_loss=0.1342 | lr=3.93029e-07 | grad_norm=3.7729 | gpu_mem(GB)=alloc:9.49 res:26.79 max_alloc:36.58 max_res:38.98 | elapsed=530.0m\n",
      "17:41:09 | INFO    | DPO step=265 | loss=0.1342 | rewards/accuracies=1.0000 | rewards/margins=2.0999 | rewards/chosen=2.8696 | rewards/rejected=0.7697\n",
      "17:48:48 | INFO    | [step 270] train_loss=0.1135 | lr=3.88615e-07 | grad_norm=1.9606 | gpu_mem(GB)=alloc:9.49 res:26.61 max_alloc:36.58 max_res:38.98 | elapsed=537.6m\n",
      "17:48:48 | INFO    | DPO step=270 | loss=0.1135 | rewards/accuracies=1.0000 | rewards/margins=2.2804 | rewards/chosen=2.8716 | rewards/rejected=0.5912\n",
      "17:56:27 | INFO    | [step 275] train_loss=0.0947 | lr=3.84137e-07 | grad_norm=3.3977 | gpu_mem(GB)=alloc:9.49 res:38.37 max_alloc:36.58 max_res:38.98 | elapsed=545.3m\n",
      "17:56:27 | INFO    | DPO step=275 | loss=0.0947 | rewards/accuracies=1.0000 | rewards/margins=2.4852 | rewards/chosen=2.9218 | rewards/rejected=0.4366\n",
      "18:04:06 | INFO    | [step 280] train_loss=0.0621 | lr=3.79598e-07 | grad_norm=1.5839 | gpu_mem(GB)=alloc:9.49 res:28.62 max_alloc:36.58 max_res:38.98 | elapsed=552.9m\n",
      "18:04:06 | INFO    | DPO step=280 | loss=0.0621 | rewards/accuracies=1.0000 | rewards/margins=2.8949 | rewards/chosen=3.0061 | rewards/rejected=0.1112\n",
      "18:11:41 | INFO    | [step 285] train_loss=0.0624 | lr=3.75e-07 | grad_norm=2.0243 | gpu_mem(GB)=alloc:9.49 res:28.98 max_alloc:36.58 max_res:38.98 | elapsed=560.5m\n",
      "18:11:41 | INFO    | DPO step=285 | loss=0.0624 | rewards/accuracies=1.0000 | rewards/margins=2.8945 | rewards/chosen=3.1176 | rewards/rejected=0.2232\n",
      "18:19:17 | INFO    | [step 290] train_loss=0.0519 | lr=3.70345e-07 | grad_norm=1.6364 | gpu_mem(GB)=alloc:9.49 res:32.18 max_alloc:36.58 max_res:38.98 | elapsed=568.1m\n",
      "18:19:17 | INFO    | DPO step=290 | loss=0.0519 | rewards/accuracies=1.0000 | rewards/margins=3.0747 | rewards/chosen=3.2094 | rewards/rejected=0.1347\n",
      "18:26:55 | INFO    | [step 295] train_loss=0.0540 | lr=3.65635e-07 | grad_norm=2.4322 | gpu_mem(GB)=alloc:9.49 res:28.10 max_alloc:36.58 max_res:38.98 | elapsed=575.8m\n",
      "18:26:55 | INFO    | DPO step=295 | loss=0.0540 | rewards/accuracies=1.0000 | rewards/margins=3.0537 | rewards/chosen=3.2081 | rewards/rejected=0.1544\n",
      "18:34:36 | INFO    | [step 300] train_loss=0.0484 | lr=3.60872e-07 | grad_norm=4.5338 | gpu_mem(GB)=alloc:9.49 res:35.69 max_alloc:36.58 max_res:38.98 | elapsed=583.4m\n",
      "18:34:36 | INFO    | DPO step=300 | loss=0.0484 | rewards/accuracies=1.0000 | rewards/margins=3.1524 | rewards/chosen=3.2068 | rewards/rejected=0.0545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 344\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:58:47 | INFO    | [step 300] eval_loss=0.0407 | gpu_mem(GB)=alloc:9.49 res:35.14 max_alloc:36.58 max_res:38.98 | elapsed=607.6m\n",
      "18:58:47 | INFO    | DPO[EVAL] step=300\n",
      "18:58:47 | INFO    | EarlyStop(metric=eval_loss): improved from 0.162448 to 0.040745\n",
      "19:06:27 | INFO    | [step 305] train_loss=0.0389 | lr=3.56058e-07 | grad_norm=1.4419 | gpu_mem(GB)=alloc:9.49 res:27.41 max_alloc:36.58 max_res:38.98 | elapsed=615.3m\n",
      "19:06:27 | INFO    | DPO step=305 | loss=0.0389 | rewards/accuracies=1.0000 | rewards/margins=3.3518 | rewards/chosen=3.2306 | rewards/rejected=-0.1212\n",
      "19:14:01 | INFO    | [step 310] train_loss=0.0305 | lr=3.51196e-07 | grad_norm=0.7915 | gpu_mem(GB)=alloc:9.49 res:24.73 max_alloc:36.58 max_res:38.98 | elapsed=622.8m\n",
      "19:14:01 | INFO    | DPO step=310 | loss=0.0305 | rewards/accuracies=1.0000 | rewards/margins=3.6150 | rewards/chosen=3.3113 | rewards/rejected=-0.3038\n",
      "19:21:44 | INFO    | [step 315] train_loss=0.0312 | lr=3.46288e-07 | grad_norm=0.8074 | gpu_mem(GB)=alloc:9.49 res:32.55 max_alloc:36.58 max_res:38.98 | elapsed=630.6m\n",
      "19:21:44 | INFO    | DPO step=315 | loss=0.0312 | rewards/accuracies=1.0000 | rewards/margins=3.5797 | rewards/chosen=3.3479 | rewards/rejected=-0.2318\n",
      "19:29:26 | INFO    | [step 320] train_loss=0.0265 | lr=3.41335e-07 | grad_norm=0.9619 | gpu_mem(GB)=alloc:9.49 res:20.54 max_alloc:36.58 max_res:38.98 | elapsed=638.3m\n",
      "19:29:26 | INFO    | DPO step=320 | loss=0.0265 | rewards/accuracies=1.0000 | rewards/margins=3.7092 | rewards/chosen=3.3491 | rewards/rejected=-0.3601\n",
      "19:36:57 | INFO    | [step 325] train_loss=0.0218 | lr=3.36341e-07 | grad_norm=0.8394 | gpu_mem(GB)=alloc:9.49 res:23.31 max_alloc:36.58 max_res:38.98 | elapsed=645.8m\n",
      "19:36:57 | INFO    | DPO step=325 | loss=0.0218 | rewards/accuracies=1.0000 | rewards/margins=3.9311 | rewards/chosen=3.4222 | rewards/rejected=-0.5090\n",
      "19:44:34 | INFO    | [step 330] train_loss=0.0255 | lr=3.31308e-07 | grad_norm=0.5425 | gpu_mem(GB)=alloc:9.49 res:32.49 max_alloc:36.58 max_res:38.98 | elapsed=653.4m\n",
      "19:44:34 | INFO    | DPO step=330 | loss=0.0255 | rewards/accuracies=1.0000 | rewards/margins=3.8787 | rewards/chosen=3.4462 | rewards/rejected=-0.4325\n",
      "19:52:19 | INFO    | [step 335] train_loss=0.0197 | lr=3.26237e-07 | grad_norm=0.6869 | gpu_mem(GB)=alloc:9.49 res:16.69 max_alloc:36.58 max_res:38.98 | elapsed=661.2m\n",
      "19:52:19 | INFO    | DPO step=335 | loss=0.0197 | rewards/accuracies=1.0000 | rewards/margins=4.1217 | rewards/chosen=3.5109 | rewards/rejected=-0.6108\n",
      "19:59:52 | INFO    | [step 340] train_loss=0.0195 | lr=3.21132e-07 | grad_norm=0.7195 | gpu_mem(GB)=alloc:9.49 res:15.91 max_alloc:36.58 max_res:38.98 | elapsed=668.7m\n",
      "19:59:52 | INFO    | DPO step=340 | loss=0.0195 | rewards/accuracies=1.0000 | rewards/margins=4.1407 | rewards/chosen=3.5007 | rewards/rejected=-0.6399\n",
      "20:07:33 | INFO    | [step 345] train_loss=0.0156 | lr=3.15994e-07 | grad_norm=0.4098 | gpu_mem(GB)=alloc:9.49 res:27.88 max_alloc:36.58 max_res:38.98 | elapsed=676.4m\n",
      "20:07:33 | INFO    | DPO step=345 | loss=0.0156 | rewards/accuracies=1.0000 | rewards/margins=4.2836 | rewards/chosen=3.5070 | rewards/rejected=-0.7766\n",
      "20:15:05 | INFO    | [step 350] train_loss=0.0156 | lr=3.10826e-07 | grad_norm=0.7903 | gpu_mem(GB)=alloc:9.49 res:37.04 max_alloc:36.58 max_res:38.98 | elapsed=683.9m\n",
      "20:15:05 | INFO    | DPO step=350 | loss=0.0156 | rewards/accuracies=1.0000 | rewards/margins=4.3069 | rewards/chosen=3.5274 | rewards/rejected=-0.7795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 344\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:39:17 | INFO    | [step 350] eval_loss=0.0140 | gpu_mem(GB)=alloc:9.49 res:35.14 max_alloc:36.58 max_res:38.98 | elapsed=708.1m\n",
      "20:39:17 | INFO    | DPO[EVAL] step=350\n",
      "20:39:17 | INFO    | EarlyStop(metric=eval_loss): improved from 0.040745 to 0.014001\n",
      "20:47:01 | INFO    | [step 355] train_loss=0.0135 | lr=3.0563e-07 | grad_norm=0.4110 | gpu_mem(GB)=alloc:9.49 res:19.50 max_alloc:36.58 max_res:38.98 | elapsed=715.8m\n",
      "20:47:01 | INFO    | DPO step=355 | loss=0.0135 | rewards/accuracies=1.0000 | rewards/margins=4.3893 | rewards/chosen=3.5226 | rewards/rejected=-0.8667\n",
      "20:54:38 | INFO    | [step 360] train_loss=0.0131 | lr=3.00409e-07 | grad_norm=0.2999 | gpu_mem(GB)=alloc:9.49 res:32.63 max_alloc:36.58 max_res:38.98 | elapsed=723.5m\n",
      "20:54:38 | INFO    | DPO step=360 | loss=0.0131 | rewards/accuracies=1.0000 | rewards/margins=4.4300 | rewards/chosen=3.5277 | rewards/rejected=-0.9023\n",
      "21:02:11 | INFO    | [step 365] train_loss=0.0146 | lr=2.95165e-07 | grad_norm=0.3899 | gpu_mem(GB)=alloc:9.49 res:24.86 max_alloc:36.58 max_res:38.98 | elapsed=731.0m\n",
      "21:02:11 | INFO    | DPO step=365 | loss=0.0146 | rewards/accuracies=1.0000 | rewards/margins=4.3695 | rewards/chosen=3.5444 | rewards/rejected=-0.8251\n",
      "21:09:58 | INFO    | [step 370] train_loss=0.0132 | lr=2.899e-07 | grad_norm=0.3325 | gpu_mem(GB)=alloc:9.49 res:23.72 max_alloc:36.58 max_res:38.98 | elapsed=738.8m\n",
      "21:09:58 | INFO    | DPO step=370 | loss=0.0132 | rewards/accuracies=1.0000 | rewards/margins=4.4296 | rewards/chosen=3.5524 | rewards/rejected=-0.8773\n",
      "21:17:36 | INFO    | [step 375] train_loss=0.0110 | lr=2.84617e-07 | grad_norm=0.3992 | gpu_mem(GB)=alloc:9.49 res:31.55 max_alloc:36.58 max_res:38.98 | elapsed=746.4m\n",
      "21:17:36 | INFO    | DPO step=375 | loss=0.0110 | rewards/accuracies=1.0000 | rewards/margins=4.6601 | rewards/chosen=3.5792 | rewards/rejected=-1.0809\n",
      "21:25:19 | INFO    | [step 380] train_loss=0.0107 | lr=2.79318e-07 | grad_norm=0.4295 | gpu_mem(GB)=alloc:9.49 res:31.69 max_alloc:36.58 max_res:38.98 | elapsed=754.2m\n",
      "21:25:19 | INFO    | DPO step=380 | loss=0.0107 | rewards/accuracies=1.0000 | rewards/margins=4.7156 | rewards/chosen=3.6141 | rewards/rejected=-1.1014\n",
      "21:32:57 | INFO    | [step 385] train_loss=0.0096 | lr=2.74006e-07 | grad_norm=0.3645 | gpu_mem(GB)=alloc:9.49 res:35.31 max_alloc:36.58 max_res:38.98 | elapsed=761.8m\n",
      "21:32:57 | INFO    | DPO step=385 | loss=0.0096 | rewards/accuracies=1.0000 | rewards/margins=4.7753 | rewards/chosen=3.5999 | rewards/rejected=-1.1754\n",
      "21:40:07 | INFO    | [step 390] train_loss=0.0092 | lr=2.68683e-07 | grad_norm=0.5314 | gpu_mem(GB)=alloc:9.49 res:33.02 max_alloc:36.58 max_res:38.98 | elapsed=769.0m\n",
      "21:40:07 | INFO    | DPO step=390 | loss=0.0092 | rewards/accuracies=1.0000 | rewards/margins=4.7924 | rewards/chosen=3.6295 | rewards/rejected=-1.1628\n",
      "21:47:43 | INFO    | [step 395] train_loss=0.0080 | lr=2.63351e-07 | grad_norm=0.3152 | gpu_mem(GB)=alloc:9.49 res:26.24 max_alloc:36.58 max_res:38.98 | elapsed=776.6m\n",
      "21:47:43 | INFO    | DPO step=395 | loss=0.0080 | rewards/accuracies=1.0000 | rewards/margins=4.9883 | rewards/chosen=3.6913 | rewards/rejected=-1.2969\n",
      "21:55:12 | INFO    | [step 400] train_loss=0.0082 | lr=2.58013e-07 | grad_norm=0.3015 | gpu_mem(GB)=alloc:9.49 res:28.80 max_alloc:36.58 max_res:38.98 | elapsed=784.0m\n",
      "21:55:12 | INFO    | DPO step=400 | loss=0.0082 | rewards/accuracies=1.0000 | rewards/margins=4.8923 | rewards/chosen=3.6695 | rewards/rejected=-1.2228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 344\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:19:22 | INFO    | [step 400] eval_loss=0.0075 | gpu_mem(GB)=alloc:9.49 res:35.14 max_alloc:36.58 max_res:38.98 | elapsed=808.2m\n",
      "22:19:22 | INFO    | DPO[EVAL] step=400\n",
      "22:19:22 | INFO    | EarlyStop(metric=eval_loss): no improvement (current=0.007533 best=0.014001) | bad=1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/checkpoint-400\n",
      "chat template saved in MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/checkpoint-400/chat_template.jinja\n",
      "tokenizer config file saved in MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/checkpoint-400/special_tokens_map.json\n",
      "/opt/conda/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:27:18 | INFO    | [step 405] train_loss=0.0084 | lr=2.52671e-07 | grad_norm=0.4605 | gpu_mem(GB)=alloc:9.49 res:20.13 max_alloc:36.58 max_res:38.98 | elapsed=816.1m\n",
      "22:27:18 | INFO    | DPO step=405 | loss=0.0084 | rewards/accuracies=1.0000 | rewards/margins=4.9592 | rewards/chosen=3.6756 | rewards/rejected=-1.2837\n",
      "22:34:57 | INFO    | [step 410] train_loss=0.0065 | lr=2.47329e-07 | grad_norm=0.2045 | gpu_mem(GB)=alloc:9.49 res:29.38 max_alloc:36.58 max_res:38.98 | elapsed=823.8m\n",
      "22:34:57 | INFO    | DPO step=410 | loss=0.0065 | rewards/accuracies=1.0000 | rewards/margins=5.1425 | rewards/chosen=3.6872 | rewards/rejected=-1.4553\n",
      "22:42:39 | INFO    | [step 415] train_loss=0.0083 | lr=2.41987e-07 | grad_norm=0.2696 | gpu_mem(GB)=alloc:9.49 res:22.86 max_alloc:36.58 max_res:38.98 | elapsed=831.5m\n",
      "22:42:39 | INFO    | DPO step=415 | loss=0.0083 | rewards/accuracies=1.0000 | rewards/margins=4.8818 | rewards/chosen=3.6779 | rewards/rejected=-1.2039\n",
      "22:50:18 | INFO    | [step 420] train_loss=0.0065 | lr=2.36649e-07 | grad_norm=0.3531 | gpu_mem(GB)=alloc:9.49 res:33.27 max_alloc:36.58 max_res:38.98 | elapsed=839.1m\n",
      "22:50:18 | INFO    | DPO step=420 | loss=0.0065 | rewards/accuracies=1.0000 | rewards/margins=5.1303 | rewards/chosen=3.6985 | rewards/rejected=-1.4319\n",
      "22:57:49 | INFO    | [step 425] train_loss=0.0061 | lr=2.31317e-07 | grad_norm=0.3451 | gpu_mem(GB)=alloc:9.49 res:32.71 max_alloc:36.58 max_res:38.98 | elapsed=846.7m\n",
      "22:57:49 | INFO    | DPO step=425 | loss=0.0061 | rewards/accuracies=1.0000 | rewards/margins=5.2161 | rewards/chosen=3.6942 | rewards/rejected=-1.5219\n",
      "23:05:22 | INFO    | [step 430] train_loss=0.0054 | lr=2.25994e-07 | grad_norm=0.2295 | gpu_mem(GB)=alloc:9.49 res:38.60 max_alloc:36.58 max_res:38.98 | elapsed=854.2m\n",
      "23:05:22 | INFO    | DPO step=430 | loss=0.0054 | rewards/accuracies=1.0000 | rewards/margins=5.3637 | rewards/chosen=3.7475 | rewards/rejected=-1.6162\n",
      "23:13:02 | INFO    | [step 435] train_loss=0.0064 | lr=2.20682e-07 | grad_norm=0.2739 | gpu_mem(GB)=alloc:9.49 res:12.59 max_alloc:36.58 max_res:38.98 | elapsed=861.9m\n",
      "23:13:02 | INFO    | DPO step=435 | loss=0.0064 | rewards/accuracies=1.0000 | rewards/margins=5.1654 | rewards/chosen=3.6861 | rewards/rejected=-1.4793\n",
      "23:20:41 | INFO    | [step 440] train_loss=0.0071 | lr=2.15383e-07 | grad_norm=0.1714 | gpu_mem(GB)=alloc:9.49 res:36.59 max_alloc:36.58 max_res:38.98 | elapsed=869.5m\n",
      "23:20:41 | INFO    | DPO step=440 | loss=0.0071 | rewards/accuracies=1.0000 | rewards/margins=5.2474 | rewards/chosen=3.7270 | rewards/rejected=-1.5204\n",
      "23:28:18 | INFO    | [step 445] train_loss=0.0059 | lr=2.101e-07 | grad_norm=0.1591 | gpu_mem(GB)=alloc:9.49 res:16.87 max_alloc:36.58 max_res:38.98 | elapsed=877.1m\n",
      "23:35:59 | INFO    | [step 450] train_loss=0.0055 | lr=2.04835e-07 | grad_norm=0.2494 | gpu_mem(GB)=alloc:9.49 res:29.66 max_alloc:36.58 max_res:38.98 | elapsed=884.8m\n",
      "23:35:59 | INFO    | DPO step=450 | loss=0.0055 | rewards/accuracies=1.0000 | rewards/margins=5.3194 | rewards/chosen=3.7219 | rewards/rejected=-1.5975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 344\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:09 | INFO    | [step 450] eval_loss=0.0053 | gpu_mem(GB)=alloc:9.49 res:35.14 max_alloc:36.58 max_res:38.98 | elapsed=909.0m\n",
      "00:00:09 | INFO    | DPO[EVAL] step=450\n",
      "00:00:09 | INFO    | EarlyStop(metric=eval_loss): no improvement (current=0.005330 best=0.014001) | bad=2/2\n",
      "00:00:09 | WARNING | EarlyStop(metric=eval_loss): STOP training (patience reached).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:09 | INFO    | [step 450] tokens/sec=0.0 | gpu_mem(GB)=alloc:9.49 res:35.14 max_alloc:36.58 max_res:38.98 | elapsed=909.0m\n",
      "00:00:09 | INFO    | DPO step=450\n",
      "00:00:09 | INFO    | DPOMetricsCallback finished | csv=MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/dpo_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chat template saved in MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/tokenizer/chat_template.jinja\n",
      "tokenizer config file saved in MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/tokenizer/tokenizer_config.json\n",
      "Special tokens file saved in MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/tokenizer/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:17 | INFO    | DPO Artifacts: {'out_dir': 'MamayLM-Gemma-3-12b_QLoRA_SFT_DPO', 'lora_adapter_dir': 'MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/lora_adapter', 'tokenizer_dir': 'MamayLM-Gemma-3-12b_QLoRA_SFT_DPO/tokenizer'}\n",
      "00:00:17 | INFO    | === DPO RUN END ===\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dpo_artifacts = run_dpo(\n",
    "    cfg_dpo,\n",
    "    do_ab_sanity=False,                 # optional control the json format - recommended \n",
    "    ab_val_jsonl=\"ft_datasets/sft_val.jsonl\",   # if do_ab_sanity take the same from SFT\n",
    "    ab_indices = list(range(15)),\n",
    "    # ab_indices=[0, 1, 2, 10, 25, 50, 100, 150, 200, 250, 300],\n",
    "    dataset_limits=(None, None),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967cd5c-6c57-4362-b708-d783019eafde",
   "metadata": {},
   "source": [
    "## 2️⃣ DPO — Direct Preference Optimization\n",
    "\n",
    "### 🎯 Goal\n",
    "\n",
    "Teach the model to **prefer the correct tariff**,  \n",
    "when the format and style are already stable (after SFT).\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Key metrics (must-watch)\n",
    "\n",
    "#### 🔹 `rewards/accuracies`\n",
    "- normal range: **0.55–0.75**\n",
    "- ❌ bad: **1.0 already at step 10–20**  \n",
    "  (a sign of truncation or style mismatch)\n",
    "\n",
    "#### 🔹 `rewards/margins`\n",
    "- should be **> 0**\n",
    "- good: grows slowly\n",
    "- ❌ bad: quickly exceeds `10`\n",
    "\n",
    "#### 🔹 `rewards/chosen > rewards/rejected`\n",
    "- should be **always true**\n",
    "- if not → DPO is not working\n",
    "\n",
    "#### 🔹 `loss`\n",
    "- does not have to go to 0\n",
    "- ❌ `loss ≈ 0` + `accuracy = 1.0` → over-optimization\n",
    "\n",
    "---\n",
    "\n",
    "### 🎛️ Main knobs (DPO)\n",
    "\n",
    "| Parameter | What it controls | When to change |\n",
    "|---------|------------------|---------------|\n",
    "| `beta` | preference strength | ↓ if accuracy quickly reaches 1.0 |\n",
    "| `learning_rate` | adaptation speed | ↓ if margins “blow up” |\n",
    "| `max_seq_len` | full context | ↑ if truncation occurs |\n",
    "| `max_prompt_length` | prompt size | ↑ if `facts` are large |\n",
    "| `max_steps` | overfitting control | ↓ if the model “wins” too fast |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Typical symptoms and fixes (DPO)\n",
    "\n",
    "| Symptom | Likely cause | Fix |\n",
    "|------|--------------|-----|\n",
    "| accuracy = 1.0 from step 20 | truncated prompt | `max_seq_len = 4096` |\n",
    "| margins > 10 | style mismatch | align chosen/rejected |\n",
    "| broken JSON | beta too high | `beta = 0.03–0.05` |\n",
    "| degradation on new cases | overfitting | ↓ LR, ↓ max_steps |\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Recommended production order\n",
    "\n",
    "1. **SFT**\n",
    "   - stable JSON\n",
    "   - correct language and structure\n",
    "2. **DPO**\n",
    "   - correct tariff selection\n",
    "   - control via rewards/accuracies\n",
    "3. **A/B sanity + business metrics**\n",
    "   - tariff accuracy\n",
    "   - JSON validity\n",
    "   - `tariffId ∈ avail`\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Minimal checklist before “OK for prod”\n",
    "\n",
    "- [ ] JSON parse rate = 100%\n",
    "- [ ] `tariffId` is always valid\n",
    "- [ ] DPO accuracy does not jump to 1.0 within 10–20 steps\n",
    "- [ ] rewards/margins < ~8–10\n",
    "- [ ] stability on unseen validation data\n",
    "\n",
    "---\n",
    "\n",
    "> 💡 Tip:  \n",
    "> If DPO still feels “too easy” after fixing truncation —  \n",
    "> the problem is almost always **style mismatch between chosen and rejected**,  \n",
    "> not the hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751fdcbf-17d4-4a6a-851b-76661cc40c10",
   "metadata": {},
   "source": [
    "## DPO metrics: what they mean & how to tune them (Tariff Recommender)\n",
    "\n",
    "This notebook logs **DPOTrainer** metrics (pairwise preference learning).  \n",
    "For each training row we have the same `prompt` and two answers:\n",
    "- **chosen** = desired recommendation (e.g., matches real customer migration / label)\n",
    "- **rejected** = plausible but undesired recommendation\n",
    "\n",
    "DPO trains the model to make **chosen more likely than rejected** for the same prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### Key logged metrics (what they mean)\n",
    "\n",
    "#### 1) `loss`\n",
    "- The DPO objective value. Lower is usually better *during* training.\n",
    "- Use it mainly to detect instability (spikes/divergence).\n",
    "- **Do not optimize loss alone** — you can overfit even when loss keeps decreasing.\n",
    "\n",
    "**If loss is unstable / spikes:**\n",
    "- Reduce `learning_rate`\n",
    "- Increase `beta` **only if** accuracy is stuck ~0.5 (rare); otherwise keep beta small\n",
    "- Consider gradient clipping (if available) and/or reduce batch noise (increase grad_accum)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) `rewards/accuracies`\n",
    "- Fraction of pairs where:\n",
    "  \\[\n",
    "  \\log P(\\text{chosen}|\\text{prompt}) > \\log P(\\text{rejected}|\\text{prompt})\n",
    "  \\]\n",
    "- Interpretation:\n",
    "  - `0.50` ≈ random preference\n",
    "  - `0.70–0.85` = good learning\n",
    "  - `>0.90` = strong preference alignment\n",
    "  - `1.00` = model always prefers chosen (watch for over-optimization)\n",
    "\n",
    "**If accuracy is low (≤0.6):**\n",
    "- Check data quality (chosen/rejected consistency)\n",
    "- Increase training steps slightly\n",
    "- Increase `beta` a bit (e.g., `0.05 → 0.07`) **carefully**\n",
    "- Ensure `max_prompt_length/max_length` prevents truncation (truncation kills preference signal)\n",
    "\n",
    "**If accuracy hits 1.0 too fast:**\n",
    "- You are likely in an easy regime → risk of overfitting the preference pairs\n",
    "- Use fewer steps and/or smaller `learning_rate`\n",
    "- Consider harder negatives (better rejected answers)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3) `rewards/margins`\n",
    "- Average preference margin:\n",
    "  \\[\n",
    "  \\text{margin} = \\log P(\\text{chosen}) - \\log P(\\text{rejected})\n",
    "  \\]\n",
    "- Interpretation:\n",
    "  - `~0` = model is unsure\n",
    "  - `1–5` = healthy confidence\n",
    "  - `>10` = very confident\n",
    "  - `>>10` (e.g., 15–20+) = can indicate **over-optimization** (model becomes too “certain”)\n",
    "\n",
    "**If margins grow very large early:**\n",
    "- Reduce `max_steps` / epochs\n",
    "- Reduce `learning_rate` (e.g., `1e-5 → 5e-6`)\n",
    "- Reduce `beta` (makes updates less aggressive)\n",
    "\n",
    "**If margins stay near 0:**\n",
    "- Increase steps slightly\n",
    "- Slightly increase `beta` (e.g., `0.03 → 0.05`)\n",
    "- Verify your rejected answers are meaningfully different (hard negatives help)\n",
    "\n",
    "---\n",
    "\n",
    "#### 4) `rewards/chosen` and `rewards/rejected`\n",
    "- These are model “scores” for each side (often derived from log-probs).\n",
    "- What you want to see:\n",
    "  - `chosen` trending up\n",
    "  - `rejected` trending down\n",
    "  - Their difference aligns with `rewards/margins`\n",
    "\n",
    "**If both go up together:**\n",
    "- You may be optimizing style rather than preference separation\n",
    "- Consider stronger negatives / improve rejected construction\n",
    "- Ensure loss is computed on the full response tokens (no masking bug)\n",
    "\n",
    "---\n",
    "\n",
    "#### 5) `learning_rate`\n",
    "- The current LR. If logs show `0.0000`, your logger may be reading the wrong key.\n",
    "- Make sure logging normalizes:\n",
    "  - `learning_rate` OR `lr` → log as `learning_rate`\n",
    "\n",
    "---\n",
    "\n",
    "### What to tune (control knobs)\n",
    "\n",
    "#### A) `learning_rate` (most important)\n",
    "- **DPO should use smaller LR than SFT.**\n",
    "- Safe defaults:\n",
    "  - `1e-5` (`0.00001`) for DPO\n",
    "  - `5e-6` (`0.000005`) if margins explode or accuracy hits 1.0 too fast\n",
    "\n",
    "Symptoms → Fix:\n",
    "- margins jump fast / accuracy → 1.0 quickly → **lower LR**\n",
    "- loss noisy / spikes → **lower LR**\n",
    "- no learning (accuracy ~0.5) → slightly higher LR or more steps (but first check data)\n",
    "\n",
    "---\n",
    "\n",
    "#### B) `beta` (aggressiveness of preference push)\n",
    "- Think of `beta` as how strongly we force chosen > rejected.\n",
    "- Safe starting point: `beta = 0.05`\n",
    "\n",
    "Symptoms → Fix:\n",
    "- accuracy stuck low → increase `beta` a bit (e.g., `0.05 → 0.07`)\n",
    "- margins explode / overconfident → reduce `beta` (e.g., `0.05 → 0.03`)\n",
    "\n",
    "---\n",
    "\n",
    "#### C) `max_steps` / epochs (how long you train)\n",
    "- DPO often converges fast.\n",
    "- Watch for:\n",
    "  - accuracy near 1.0 + margins rising fast → stop early\n",
    "\n",
    "Practical guidance:\n",
    "- Start with **200–400 steps** total (for ~3k rows with grad_accum=8 this is often enough)\n",
    "- Use early stopping on `eval_loss` and/or a custom rule on margins/accuracy if available.\n",
    "\n",
    "---\n",
    "\n",
    "#### D) `max_length` / `max_prompt_length` (avoid truncation)\n",
    "If prompt p95 ~3000 and answers ~300–400 tokens:\n",
    "- Use `max_length = 4096`\n",
    "- Use `max_prompt_length ~ 3500–3600` (leave room for response)\n",
    "\n",
    "Symptoms of truncation:\n",
    "- accuracy fails to improve\n",
    "- margins stay near 0\n",
    "- training becomes noisy\n",
    "\n",
    "---\n",
    "\n",
    "### What to watch for in practice (quick checklist)\n",
    "\n",
    "✅ Healthy DPO run:\n",
    "- `rewards/accuracies` rises from ~0.5 → 0.8–0.95\n",
    "- `rewards/margins` increases to a moderate range (often 2–10)\n",
    "- `chosen` up, `rejected` down\n",
    "- eval metrics stabilize without degrading output format (JSON validity)\n",
    "\n",
    "⚠️ Over-optimization signs:\n",
    "- `rewards/accuracies` → 1.0 very early\n",
    "- `rewards/margins` keeps climbing to very high values (e.g., >10–15) while eval stops improving\n",
    "- output quality issues (e.g., worse JSON compliance or less robust behavior)\n",
    "\n",
    "**Fix over-optimization:**\n",
    "- fewer steps\n",
    "- lower `learning_rate`\n",
    "- lower `beta`\n",
    "\n",
    "---\n",
    "\n",
    "### Optional: add a format metric (recommended for production)\n",
    "Besides DPO metrics, track a **task-format KPI** on a fixed subset (A/B sanity):\n",
    "- `valid_json_rate`\n",
    "- tariff field presence: `tariffId`, `templateId`, etc.\n",
    "\n",
    "If `valid_json_rate` drops while DPO metrics “improve”:\n",
    "- prefer stopping early / reduce aggressiveness\n",
    "- keep generation deterministic during evaluation (`do_sample=False`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f832b-9ca3-46d4-a540-731201cd83d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
