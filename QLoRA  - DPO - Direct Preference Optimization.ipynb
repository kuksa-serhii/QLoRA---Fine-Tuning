{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b87b60-fe40-411d-86ac-ba09eb35cae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100-SXM4-40GB\n",
      "allocated: 0.0 MB\n",
      "reserved: 0.0 MB\n",
      "allocated: 0.0 MB\n",
      "reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1 ‚Äî ENV (MUST be first, before torch/transformers)\n",
    "# =========================\n",
    "from ft_pipeline.env import apply_env\n",
    "apply_env()\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n",
    "\n",
    "print(\"allocated:\", torch.cuda.memory_allocated()/1024**2, \"MB\")\n",
    "print(\"reserved:\",  torch.cuda.memory_reserved()/1024**2, \"MB\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"allocated:\", torch.cuda.memory_allocated()/1024**2, \"MB\")\n",
    "print(\"reserved:\",  torch.cuda.memory_reserved()/1024**2, \"MB\")\n",
    "\n",
    "\n",
    "import logging\n",
    "from ft_pipeline.logger import setup_logger\n",
    "from ft_pipeline.config import FTConfig\n",
    "from ft_pipeline.run_sft import run_finetune\n",
    "from ft_pipeline.config import DPOCfg\n",
    "from ft_pipeline.run_dpo import run_dpo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc98c0b5-88df-419b-850d-8fc420dafdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPOCfg(model_id='/home/jovyan/ai-models/MamayLM-Gemma-3-12B', sft_adapter_dir='MamayLM-Gemma-3-12b_QLoRA_SFT/lora_adapter', dpo_train_jsonl='ft_datasets/dpo_train.jsonl', dpo_val_jsonl='ft_datasets/dpo_val.jsonl', out_dir='outputs_mamay12b_qlora_dpo', max_seq_len=4096, per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=8, learning_rate=5e-07, weight_decay=0.05, num_train_epochs=1.0, max_steps=400, warmup_ratio=0.05, lr_scheduler_type='cosine', logging_steps=5, eval_steps=50, save_steps=200, save_total_limit=2, beta=0.03, use_bf16=True, use_fp16=False, load_in_4bit=True, attn_implementation='sdpa', report_to='none', optim='paged_adamw_8bit', max_new_tokens_eval=512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Logger ft_pipeline (INFO)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell ‚Äî DPO Config\n",
    "# =========================\n",
    "# DPO (Direct Preference Optimization) after SFT (QLoRA LoRA-adapter)\n",
    "# MamayLM-Gemma-3-12B (–ª–æ–∫–∞–ª—å–Ω–∏–π —à–ª—è—Ö)\n",
    "# GPU: A100 40GB, BF16, QLoRA 4-bit\n",
    "\n",
    "from ft_pipeline.config import DPOCfg\n",
    "\n",
    "cfg_dpo = DPOCfg(\n",
    "    # ==========================================================\n",
    "    # BASE MODEL + CONTINUATION FROM SFT\n",
    "    # ==========================================================\n",
    "    model_id=\"/home/jovyan/ai-models/MamayLM-Gemma-3-12B\",               # path or HF repo id base model\n",
    "    \n",
    "    sft_adapter_dir=\"MamayLM-Gemma-3-12b_QLoRA_SFT/lora_adapter\",           # LoRA-adapter after SFT,\n",
    "    # sft_adapter_dir=None,                                              # if DPO from base\n",
    "    \n",
    "    dpo_train_jsonl=\"ft_datasets/dpo_train.jsonl\",                      # train dataset in JSONL\n",
    "    dpo_val_jsonl=\"ft_datasets/dpo_val.jsonl\",                          # validation dataset in JSONL\n",
    "\n",
    "    \n",
    "    out_dir=\"outputs_mamay12b_qlora_dpo\",\n",
    "    # out_dir=\"MamayLM-Gemma-3-12b_QLoRA_SFT_DPO\",\n",
    "    # out_dir=\"MamayLM-Gemma-3-12b_QLoRA_DPO\",\n",
    "    \n",
    "    # ==========================================================\n",
    "    # SEQUENCE / BATCHING\n",
    "    # ==========================================================\n",
    "    max_seq_len=4096,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # effective batch = batch_size * grad_accum\n",
    "\n",
    "    # ==========================================================\n",
    "    # TRAINING SCHEDULE / OPTIM\n",
    "    # ==========================================================\n",
    "    learning_rate=0.0000005,     \n",
    "    weight_decay=0.05,        # L2 regularization \n",
    "    num_train_epochs=1.0,     # (if  max_steps are provided - will ignored)\n",
    "    max_steps=400,            # use instead - num_train_epochs\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",  #scheduler: \"cosine\", \"linear\", ...\n",
    "    logging_steps=5,\n",
    "    eval_steps=50,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # ==========================================================\n",
    "    # DPO CORE\n",
    "    # ==========================================================\n",
    "    \n",
    "    beta=0.03,   # to avoid overfit - safe start. -- if   accuracy getting to fast to 1.0 - change it  ‚Üí 0.03\n",
    "\n",
    "    use_bf16=True,            \n",
    "    use_fp16=False,\n",
    "    load_in_4bit=True,         # QLoRA (bitsandbytes 4-bit)\n",
    "    attn_implementation=\"sdpa\",   \n",
    "    optim=\"paged_adamw_8bit\",  # bitsandbytes to reduce the memory\n",
    "    report_to=\"none\",\n",
    "    max_new_tokens_eval=512,  #  in A/B sanity (before/after)\n",
    ")\n",
    "\n",
    "print(cfg_dpo)\n",
    "\n",
    "setup_logger(level=logging.INFO, log_file=f\"{cfg_dpo.out_dir}/ft_run_dpo.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649ae66-d7fa-40e3-9248-c6af8c8976da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:35:19 | INFO    | === DPO RUN START ===\n",
      "08:35:19 | INFO    | CUDA available=True\n",
      "08:35:19 | INFO    | CUDA device=NVIDIA A100-SXM4-40GB\n",
      "08:35:19 | INFO    | Loading tokenizer: /home/jovyan/ai-models/MamayLM-Gemma-3-12B\n",
      "08:35:21 | INFO    | Tokenizer loaded\n",
      "08:35:21 | INFO    | Loading DPO datasets\n",
      "08:35:21 | INFO    |   train: ft_datasets/dpo_train.jsonl\n",
      "08:35:21 | INFO    |   val:   ft_datasets/dpo_val.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1167b72d483e438f973a8ae0b561b4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966f1e9bca4d4ccd8992d30d65b1761b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319223bf282e46e98ef0d511f01a7eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c71aa681a24ffb9005225f9bd5a1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:35:46 | INFO    | DPO dataset ready | train=3094 | val=344\n",
      "08:35:46 | INFO    | Loading base model (QLoRA)\n",
      "08:35:46 | INFO    |   model_id: /home/jovyan/ai-models/MamayLM-Gemma-3-12B\n",
      "08:35:46 | INFO    |   dtype: torch.bfloat16\n",
      "08:35:46 | INFO    |   4bit: True\n",
      "08:35:46 | INFO    |   attn_implementation: sdpa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cafff96270c4ba580356268996c5ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:36:52 | INFO    | Base model loaded\n",
      "08:36:52 | INFO    | Enabling gradient checkpointing\n",
      "08:36:52 | INFO    | Loading trainable LoRA adapter from: MamayLM-Gemma-3-12b_QLoRA_SFT/lora_adapter\n",
      "08:36:54 | INFO    | Trainable adapter loaded\n",
      "08:36:54 | INFO    | Trainable parameters:\n",
      "trainable params: 68,456,448 || all params: 12,255,781,488 || trainable%: 0.5586\n",
      "08:36:54 | INFO    | Building DPOConfig\n",
      "08:36:54 | INFO    |   max_seq_len=4096\n",
      "08:36:54 | INFO    |   beta=0.03\n",
      "08:36:54 | INFO    |   lr=5e-07\n",
      "08:36:54 | INFO    | Building DPOTrainer\n",
      "08:36:54 | INFO    |   train_samples=3094\n",
      "08:36:54 | INFO    |   val_samples=344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3e673a62c74378a357c5d397a030a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/3094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4358d31e27d74a7f8c2105053e6a5338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/3094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26597121cd054cdfb7791bb67a988ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a65676a60e4f9ea18a5ae07f9080ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd370b0687bf4b64803bdb5f97245aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4f42286a0049f1a59d9604d1c35913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:37:07 | INFO    | Starting DPO training‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot get num_tokens from dataloader\n",
      "skipped Embedding(4096, 1152): 4.5M params\n",
      "skipped Gemma3TextScaledWordEmbedding(262208, 3840, padding_idx=0): 964.734375M params\n",
      "skipped: 964.734375M params\n",
      "***** Running training *****\n",
      "  Num examples = 3,094\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 400\n",
      "  Number of trainable parameters = 68,456,448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:37:08 | INFO    | GPUMetricsCallback enabled\n",
      "08:37:08 | INFO    | DPOMetricsCallback enabled | csv=outputs_mamay12b_qlora_dpo/dpo_metrics.csv | every_n_steps=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [101/400 2:56:01 < 8:51:37, 0.01 it/s, Epoch 0.26/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.244100</td>\n",
       "      <td>3.214541</td>\n",
       "      <td>0.188638</td>\n",
       "      <td>3.357976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.169337</td>\n",
       "      <td>-259.113403</td>\n",
       "      <td>-68.248726</td>\n",
       "      <td>-2.019982</td>\n",
       "      <td>-3.067348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:38:42 | INFO    | [step 1] train_loss=4.3627 | lr=0 | grad_norm=40.3302 | gpu_mem(GB)=alloc:9.49 res:23.14 max_alloc:36.02 max_res:38.98 | elapsed=1.6m\n",
      "08:44:50 | INFO    | [step 5] train_loss=4.4817 | lr=1e-07 | grad_norm=46.3626 | gpu_mem(GB)=alloc:9.49 res:27.82 max_alloc:36.24 max_res:38.98 | elapsed=7.7m\n",
      "08:44:50 | INFO    | DPO step=5 | loss=4.4817 | rewards/accuracies=0.0000 | rewards/margins=-4.4692 | rewards/chosen=-1.0060 | rewards/rejected=3.4632\n",
      "08:52:30 | INFO    | [step 10] train_loss=4.4164 | lr=2.25e-07 | grad_norm=33.7282 | gpu_mem(GB)=alloc:9.49 res:27.02 max_alloc:36.28 max_res:38.98 | elapsed=15.4m\n",
      "08:52:30 | INFO    | DPO step=10 | loss=4.4164 | rewards/accuracies=0.0000 | rewards/margins=-4.4031 | rewards/chosen=-0.9108 | rewards/rejected=3.4923\n",
      "09:00:15 | INFO    | [step 15] train_loss=4.4242 | lr=3.5e-07 | grad_norm=32.3134 | gpu_mem(GB)=alloc:9.49 res:29.11 max_alloc:36.28 max_res:38.98 | elapsed=23.1m\n",
      "09:00:15 | INFO    | DPO step=15 | loss=4.4242 | rewards/accuracies=0.0000 | rewards/margins=-4.4111 | rewards/chosen=-0.9083 | rewards/rejected=3.5028\n",
      "09:07:55 | INFO    | [step 20] train_loss=4.2362 | lr=4.75e-07 | grad_norm=33.0617 | gpu_mem(GB)=alloc:9.49 res:16.87 max_alloc:36.28 max_res:38.98 | elapsed=30.8m\n",
      "09:07:55 | INFO    | DPO step=20 | loss=4.2362 | rewards/accuracies=0.0000 | rewards/margins=-4.2201 | rewards/chosen=-0.7611 | rewards/rejected=3.4590\n",
      "09:15:28 | INFO    | [step 25] train_loss=4.0487 | lr=4.99863e-07 | grad_norm=31.0731 | gpu_mem(GB)=alloc:9.49 res:35.19 max_alloc:36.28 max_res:38.98 | elapsed=38.3m\n",
      "09:15:28 | INFO    | DPO step=25 | loss=4.0487 | rewards/accuracies=0.0000 | rewards/margins=-4.0294 | rewards/chosen=-0.5946 | rewards/rejected=3.4347\n",
      "09:23:06 | INFO    | [step 30] train_loss=3.8838 | lr=4.99308e-07 | grad_norm=37.9582 | gpu_mem(GB)=alloc:9.49 res:33.82 max_alloc:36.28 max_res:38.98 | elapsed=46.0m\n",
      "09:23:06 | INFO    | DPO step=30 | loss=3.8838 | rewards/accuracies=0.0000 | rewards/margins=-3.8615 | rewards/chosen=-0.4619 | rewards/rejected=3.3995\n",
      "09:30:50 | INFO    | [step 35] train_loss=3.7445 | lr=4.98327e-07 | grad_norm=29.1970 | gpu_mem(GB)=alloc:9.49 res:26.87 max_alloc:36.49 max_res:38.98 | elapsed=53.7m\n",
      "09:30:50 | INFO    | DPO step=35 | loss=3.7445 | rewards/accuracies=0.0000 | rewards/margins=-3.7185 | rewards/chosen=-0.2606 | rewards/rejected=3.4580\n",
      "09:38:29 | INFO    | [step 40] train_loss=3.5474 | lr=4.96922e-07 | grad_norm=29.0814 | gpu_mem(GB)=alloc:9.49 res:29.03 max_alloc:36.49 max_res:38.98 | elapsed=61.4m\n",
      "09:38:29 | INFO    | DPO step=40 | loss=3.5474 | rewards/accuracies=0.0000 | rewards/margins=-3.5145 | rewards/chosen=-0.1694 | rewards/rejected=3.3451\n",
      "09:46:01 | INFO    | [step 45] train_loss=3.3868 | lr=4.95095e-07 | grad_norm=24.3794 | gpu_mem(GB)=alloc:9.49 res:20.89 max_alloc:36.49 max_res:38.98 | elapsed=68.9m\n",
      "09:46:01 | INFO    | DPO step=45 | loss=3.3868 | rewards/accuracies=0.0000 | rewards/margins=-3.3493 | rewards/chosen=-0.0351 | rewards/rejected=3.3141\n",
      "09:53:54 | INFO    | [step 50] train_loss=3.2441 | lr=4.92849e-07 | grad_norm=27.6823 | gpu_mem(GB)=alloc:9.49 res:18.54 max_alloc:36.53 max_res:38.98 | elapsed=76.8m\n",
      "09:53:54 | INFO    | DPO step=50 | loss=3.2441 | rewards/accuracies=0.0000 | rewards/margins=-3.2009 | rewards/chosen=0.1155 | rewards/rejected=3.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 344\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:18:03 | INFO    | [step 50] eval_loss=3.2145 | gpu_mem(GB)=alloc:9.49 res:35.14 max_alloc:36.53 max_res:38.98 | elapsed=100.9m\n",
      "10:18:03 | INFO    | DPO[EVAL] step=50\n",
      "10:18:03 | INFO    | EarlyStop(metric=eval_loss): improved from None to 3.214541\n",
      "10:25:48 | INFO    | [step 55] train_loss=3.2341 | lr=4.90188e-07 | grad_norm=25.2550 | gpu_mem(GB)=alloc:9.49 res:19.97 max_alloc:36.53 max_res:38.98 | elapsed=108.7m\n",
      "10:25:48 | INFO    | DPO step=55 | loss=3.2341 | rewards/accuracies=0.0000 | rewards/margins=-3.1878 | rewards/chosen=0.2325 | rewards/rejected=3.4203\n",
      "10:33:28 | INFO    | [step 60] train_loss=2.9724 | lr=4.87117e-07 | grad_norm=23.5792 | gpu_mem(GB)=alloc:9.49 res:22.39 max_alloc:36.53 max_res:38.98 | elapsed=116.3m\n",
      "10:33:28 | INFO    | DPO step=60 | loss=2.9724 | rewards/accuracies=0.0000 | rewards/margins=-2.9157 | rewards/chosen=0.3311 | rewards/rejected=3.2467\n",
      "10:41:09 | INFO    | [step 65] train_loss=2.9635 | lr=4.83641e-07 | grad_norm=22.3052 | gpu_mem(GB)=alloc:9.49 res:22.41 max_alloc:36.53 max_res:38.98 | elapsed=124.0m\n",
      "10:41:09 | INFO    | DPO step=65 | loss=2.9635 | rewards/accuracies=0.0000 | rewards/margins=-2.9041 | rewards/chosen=0.4433 | rewards/rejected=3.3474\n",
      "10:48:49 | INFO    | [step 70] train_loss=2.9124 | lr=4.79766e-07 | grad_norm=22.2818 | gpu_mem(GB)=alloc:9.49 res:37.65 max_alloc:36.53 max_res:38.98 | elapsed=131.7m\n",
      "10:48:49 | INFO    | DPO step=70 | loss=2.9124 | rewards/accuracies=0.0000 | rewards/margins=-2.8515 | rewards/chosen=0.5436 | rewards/rejected=3.3950\n",
      "10:56:25 | INFO    | [step 75] train_loss=2.6778 | lr=4.75498e-07 | grad_norm=21.2240 | gpu_mem(GB)=alloc:9.49 res:28.60 max_alloc:36.53 max_res:38.98 | elapsed=139.3m\n",
      "10:56:25 | INFO    | DPO step=75 | loss=2.6778 | rewards/accuracies=0.0000 | rewards/margins=-2.6006 | rewards/chosen=0.6471 | rewards/rejected=3.2477\n",
      "11:04:06 | INFO    | [step 80] train_loss=2.7142 | lr=4.70845e-07 | grad_norm=23.1777 | gpu_mem(GB)=alloc:9.49 res:38.84 max_alloc:36.53 max_res:38.98 | elapsed=147.0m\n",
      "11:04:06 | INFO    | DPO step=80 | loss=2.7142 | rewards/accuracies=0.0000 | rewards/margins=-2.6404 | rewards/chosen=0.7285 | rewards/rejected=3.3689\n",
      "11:11:47 | INFO    | [step 85] train_loss=2.4873 | lr=4.65814e-07 | grad_norm=21.2292 | gpu_mem(GB)=alloc:9.49 res:18.86 max_alloc:36.53 max_res:38.98 | elapsed=154.7m\n",
      "11:11:47 | INFO    | DPO step=85 | loss=2.4873 | rewards/accuracies=0.0000 | rewards/margins=-2.3912 | rewards/chosen=0.8193 | rewards/rejected=3.2105\n",
      "11:19:22 | INFO    | [step 90] train_loss=2.3484 | lr=4.60415e-07 | grad_norm=21.2663 | gpu_mem(GB)=alloc:9.49 res:12.49 max_alloc:36.53 max_res:38.98 | elapsed=162.2m\n",
      "11:19:22 | INFO    | DPO step=90 | loss=2.3484 | rewards/accuracies=0.0000 | rewards/margins=-2.2419 | rewards/chosen=0.8921 | rewards/rejected=3.1340\n",
      "11:27:02 | INFO    | [step 95] train_loss=2.3879 | lr=4.54656e-07 | grad_norm=20.9903 | gpu_mem(GB)=alloc:9.49 res:33.10 max_alloc:36.53 max_res:38.98 | elapsed=169.9m\n",
      "11:27:02 | INFO    | DPO step=95 | loss=2.3879 | rewards/accuracies=0.0000 | rewards/margins=-2.2807 | rewards/chosen=1.0051 | rewards/rejected=3.2858\n",
      "11:34:43 | INFO    | [step 100] train_loss=2.1833 | lr=4.48548e-07 | grad_norm=20.2100 | gpu_mem(GB)=alloc:9.49 res:30.50 max_alloc:36.53 max_res:38.98 | elapsed=177.6m\n",
      "11:34:43 | INFO    | DPO step=100 | loss=2.1833 | rewards/accuracies=0.0000 | rewards/margins=-2.0528 | rewards/chosen=1.0973 | rewards/rejected=3.1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 344\n",
      "  Batch size = 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dpo_artifacts = run_dpo(\n",
    "    cfg_dpo,\n",
    "    do_ab_sanity=False,                 # optional control the json format - recommended \n",
    "    ab_val_jsonl=\"ft_datasets/sft_val.jsonl\",   # if do_ab_sanity take the same from SFT\n",
    "    ab_indices = list(range(5)),\n",
    "    # ab_indices=[0, 1, 2, 10, 25, 50, 100, 150, 200, 250, 300],\n",
    "    dataset_limits=(None, None),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967cd5c-6c57-4362-b708-d783019eafde",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ DPO ‚Äî Direct Preference Optimization\n",
    "\n",
    "### üéØ Goal\n",
    "\n",
    "Teach the model to **prefer the correct tariff**,  \n",
    "when the format and style are already stable (after SFT).\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Key metrics (must-watch)\n",
    "\n",
    "#### üîπ `rewards/accuracies`\n",
    "- normal range: **0.55‚Äì0.75**\n",
    "- ‚ùå bad: **1.0 already at step 10‚Äì20**  \n",
    "  (a sign of truncation or style mismatch)\n",
    "\n",
    "#### üîπ `rewards/margins`\n",
    "- should be **> 0**\n",
    "- good: grows slowly\n",
    "- ‚ùå bad: quickly exceeds `10`\n",
    "\n",
    "#### üîπ `rewards/chosen > rewards/rejected`\n",
    "- should be **always true**\n",
    "- if not ‚Üí DPO is not working\n",
    "\n",
    "#### üîπ `loss`\n",
    "- does not have to go to 0\n",
    "- ‚ùå `loss ‚âà 0` + `accuracy = 1.0` ‚Üí over-optimization\n",
    "\n",
    "---\n",
    "\n",
    "### üéõÔ∏è Main knobs (DPO)\n",
    "\n",
    "| Parameter | What it controls | When to change |\n",
    "|---------|------------------|---------------|\n",
    "| `beta` | preference strength | ‚Üì if accuracy quickly reaches 1.0 |\n",
    "| `learning_rate` | adaptation speed | ‚Üì if margins ‚Äúblow up‚Äù |\n",
    "| `max_seq_len` | full context | ‚Üë if truncation occurs |\n",
    "| `max_prompt_length` | prompt size | ‚Üë if `facts` are large |\n",
    "| `max_steps` | overfitting control | ‚Üì if the model ‚Äúwins‚Äù too fast |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Typical symptoms and fixes (DPO)\n",
    "\n",
    "| Symptom | Likely cause | Fix |\n",
    "|------|--------------|-----|\n",
    "| accuracy = 1.0 from step 20 | truncated prompt | `max_seq_len = 4096` |\n",
    "| margins > 10 | style mismatch | align chosen/rejected |\n",
    "| broken JSON | beta too high | `beta = 0.03‚Äì0.05` |\n",
    "| degradation on new cases | overfitting | ‚Üì LR, ‚Üì max_steps |\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Recommended production order\n",
    "\n",
    "1. **SFT**\n",
    "   - stable JSON\n",
    "   - correct language and structure\n",
    "2. **DPO**\n",
    "   - correct tariff selection\n",
    "   - control via rewards/accuracies\n",
    "3. **A/B sanity + business metrics**\n",
    "   - tariff accuracy\n",
    "   - JSON validity\n",
    "   - `tariffId ‚àà avail`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Minimal checklist before ‚ÄúOK for prod‚Äù\n",
    "\n",
    "- [ ] JSON parse rate = 100%\n",
    "- [ ] `tariffId` is always valid\n",
    "- [ ] DPO accuracy does not jump to 1.0 within 10‚Äì20 steps\n",
    "- [ ] rewards/margins < ~8‚Äì10\n",
    "- [ ] stability on unseen validation data\n",
    "\n",
    "---\n",
    "\n",
    "> üí° Tip:  \n",
    "> If DPO still feels ‚Äútoo easy‚Äù after fixing truncation ‚Äî  \n",
    "> the problem is almost always **style mismatch between chosen and rejected**,  \n",
    "> not the hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751fdcbf-17d4-4a6a-851b-76661cc40c10",
   "metadata": {},
   "source": [
    "## DPO metrics: what they mean & how to tune them (Tariff Recommender)\n",
    "\n",
    "This notebook logs **DPOTrainer** metrics (pairwise preference learning).  \n",
    "For each training row we have the same `prompt` and two answers:\n",
    "- **chosen** = desired recommendation (e.g., matches real customer migration / label)\n",
    "- **rejected** = plausible but undesired recommendation\n",
    "\n",
    "DPO trains the model to make **chosen more likely than rejected** for the same prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### Key logged metrics (what they mean)\n",
    "\n",
    "#### 1) `loss`\n",
    "- The DPO objective value. Lower is usually better *during* training.\n",
    "- Use it mainly to detect instability (spikes/divergence).\n",
    "- **Do not optimize loss alone** ‚Äî you can overfit even when loss keeps decreasing.\n",
    "\n",
    "**If loss is unstable / spikes:**\n",
    "- Reduce `learning_rate`\n",
    "- Increase `beta` **only if** accuracy is stuck ~0.5 (rare); otherwise keep beta small\n",
    "- Consider gradient clipping (if available) and/or reduce batch noise (increase grad_accum)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) `rewards/accuracies`\n",
    "- Fraction of pairs where:\n",
    "  \\[\n",
    "  \\log P(\\text{chosen}|\\text{prompt}) > \\log P(\\text{rejected}|\\text{prompt})\n",
    "  \\]\n",
    "- Interpretation:\n",
    "  - `0.50` ‚âà random preference\n",
    "  - `0.70‚Äì0.85` = good learning\n",
    "  - `>0.90` = strong preference alignment\n",
    "  - `1.00` = model always prefers chosen (watch for over-optimization)\n",
    "\n",
    "**If accuracy is low (‚â§0.6):**\n",
    "- Check data quality (chosen/rejected consistency)\n",
    "- Increase training steps slightly\n",
    "- Increase `beta` a bit (e.g., `0.05 ‚Üí 0.07`) **carefully**\n",
    "- Ensure `max_prompt_length/max_length` prevents truncation (truncation kills preference signal)\n",
    "\n",
    "**If accuracy hits 1.0 too fast:**\n",
    "- You are likely in an easy regime ‚Üí risk of overfitting the preference pairs\n",
    "- Use fewer steps and/or smaller `learning_rate`\n",
    "- Consider harder negatives (better rejected answers)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3) `rewards/margins`\n",
    "- Average preference margin:\n",
    "  \\[\n",
    "  \\text{margin} = \\log P(\\text{chosen}) - \\log P(\\text{rejected})\n",
    "  \\]\n",
    "- Interpretation:\n",
    "  - `~0` = model is unsure\n",
    "  - `1‚Äì5` = healthy confidence\n",
    "  - `>10` = very confident\n",
    "  - `>>10` (e.g., 15‚Äì20+) = can indicate **over-optimization** (model becomes too ‚Äúcertain‚Äù)\n",
    "\n",
    "**If margins grow very large early:**\n",
    "- Reduce `max_steps` / epochs\n",
    "- Reduce `learning_rate` (e.g., `1e-5 ‚Üí 5e-6`)\n",
    "- Reduce `beta` (makes updates less aggressive)\n",
    "\n",
    "**If margins stay near 0:**\n",
    "- Increase steps slightly\n",
    "- Slightly increase `beta` (e.g., `0.03 ‚Üí 0.05`)\n",
    "- Verify your rejected answers are meaningfully different (hard negatives help)\n",
    "\n",
    "---\n",
    "\n",
    "#### 4) `rewards/chosen` and `rewards/rejected`\n",
    "- These are model ‚Äúscores‚Äù for each side (often derived from log-probs).\n",
    "- What you want to see:\n",
    "  - `chosen` trending up\n",
    "  - `rejected` trending down\n",
    "  - Their difference aligns with `rewards/margins`\n",
    "\n",
    "**If both go up together:**\n",
    "- You may be optimizing style rather than preference separation\n",
    "- Consider stronger negatives / improve rejected construction\n",
    "- Ensure loss is computed on the full response tokens (no masking bug)\n",
    "\n",
    "---\n",
    "\n",
    "#### 5) `learning_rate`\n",
    "- The current LR. If logs show `0.0000`, your logger may be reading the wrong key.\n",
    "- Make sure logging normalizes:\n",
    "  - `learning_rate` OR `lr` ‚Üí log as `learning_rate`\n",
    "\n",
    "---\n",
    "\n",
    "### What to tune (control knobs)\n",
    "\n",
    "#### A) `learning_rate` (most important)\n",
    "- **DPO should use smaller LR than SFT.**\n",
    "- Safe defaults:\n",
    "  - `1e-5` (`0.00001`) for DPO\n",
    "  - `5e-6` (`0.000005`) if margins explode or accuracy hits 1.0 too fast\n",
    "\n",
    "Symptoms ‚Üí Fix:\n",
    "- margins jump fast / accuracy ‚Üí 1.0 quickly ‚Üí **lower LR**\n",
    "- loss noisy / spikes ‚Üí **lower LR**\n",
    "- no learning (accuracy ~0.5) ‚Üí slightly higher LR or more steps (but first check data)\n",
    "\n",
    "---\n",
    "\n",
    "#### B) `beta` (aggressiveness of preference push)\n",
    "- Think of `beta` as how strongly we force chosen > rejected.\n",
    "- Safe starting point: `beta = 0.05`\n",
    "\n",
    "Symptoms ‚Üí Fix:\n",
    "- accuracy stuck low ‚Üí increase `beta` a bit (e.g., `0.05 ‚Üí 0.07`)\n",
    "- margins explode / overconfident ‚Üí reduce `beta` (e.g., `0.05 ‚Üí 0.03`)\n",
    "\n",
    "---\n",
    "\n",
    "#### C) `max_steps` / epochs (how long you train)\n",
    "- DPO often converges fast.\n",
    "- Watch for:\n",
    "  - accuracy near 1.0 + margins rising fast ‚Üí stop early\n",
    "\n",
    "Practical guidance:\n",
    "- Start with **200‚Äì400 steps** total (for ~3k rows with grad_accum=8 this is often enough)\n",
    "- Use early stopping on `eval_loss` and/or a custom rule on margins/accuracy if available.\n",
    "\n",
    "---\n",
    "\n",
    "#### D) `max_length` / `max_prompt_length` (avoid truncation)\n",
    "If prompt p95 ~3000 and answers ~300‚Äì400 tokens:\n",
    "- Use `max_length = 4096`\n",
    "- Use `max_prompt_length ~ 3500‚Äì3600` (leave room for response)\n",
    "\n",
    "Symptoms of truncation:\n",
    "- accuracy fails to improve\n",
    "- margins stay near 0\n",
    "- training becomes noisy\n",
    "\n",
    "---\n",
    "\n",
    "### What to watch for in practice (quick checklist)\n",
    "\n",
    "‚úÖ Healthy DPO run:\n",
    "- `rewards/accuracies` rises from ~0.5 ‚Üí 0.8‚Äì0.95\n",
    "- `rewards/margins` increases to a moderate range (often 2‚Äì10)\n",
    "- `chosen` up, `rejected` down\n",
    "- eval metrics stabilize without degrading output format (JSON validity)\n",
    "\n",
    "‚ö†Ô∏è Over-optimization signs:\n",
    "- `rewards/accuracies` ‚Üí 1.0 very early\n",
    "- `rewards/margins` keeps climbing to very high values (e.g., >10‚Äì15) while eval stops improving\n",
    "- output quality issues (e.g., worse JSON compliance or less robust behavior)\n",
    "\n",
    "**Fix over-optimization:**\n",
    "- fewer steps\n",
    "- lower `learning_rate`\n",
    "- lower `beta`\n",
    "\n",
    "---\n",
    "\n",
    "### Optional: add a format metric (recommended for production)\n",
    "Besides DPO metrics, track a **task-format KPI** on a fixed subset (A/B sanity):\n",
    "- `valid_json_rate`\n",
    "- tariff field presence: `tariffId`, `templateId`, etc.\n",
    "\n",
    "If `valid_json_rate` drops while DPO metrics ‚Äúimprove‚Äù:\n",
    "- prefer stopping early / reduce aggressiveness\n",
    "- keep generation deterministic during evaluation (`do_sample=False`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f832b-9ca3-46d4-a540-731201cd83d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
